"""
HKGraphTreeDynamic: å¢é‡æ›´æ–°ç‰ˆæœ¬çš„HKGraphTree

é›†æˆäº†EraRAG TreeGraphDynamicçš„å¢é‡æ›´æ–°æœºåˆ¶åˆ°HKGraphTreeçš„æ··åˆå›¾æ¶æ„ä¸­
"""

import asyncio
import re
import numpy as np
import random
import pickle
import os
from collections import defaultdict, deque
from typing import Any, List, Dict, Set, Tuple, Optional
from itertools import combinations
import networkx as nx
from concurrent.futures import ThreadPoolExecutor, as_completed
import faiss

from Core.Graph.HKGraphTree import HKGraphTree
from Core.Common.Logger import logger
from Core.Common.Utils import clean_str, prase_json_from_response
from Core.Schema.ChunkSchema import TextChunk
from Core.Schema.Message import Message
from Core.Prompt import GraphPrompt
from Core.Prompt.Base import TextPrompt
from Core.Schema.EntityRelation import Entity, Relationship, HK_Node
from Core.Index.EmbeddingFactory import get_rag_embedding
from Core.Common.Constants import (
    NODE_PATTERN,
    REL_PATTERN,
    GRAPH_FIELD_SEP
)
from Core.Storage.HKGraphTreeStorage import HKGraphTreeStorage
from Core.Utils.WAT import WATAnnotation
import requests
from Core.Common.Constants import GCUBE_TOKEN
from tqdm import tqdm


class HKNodeAux:
    """
    HKå›¾èŠ‚ç‚¹çš„è¾…åŠ©ä¿¡æ¯ç±»ï¼Œç”¨äºå¢é‡æ›´æ–°ç®¡ç†
    ç±»ä¼¼äºTreeGraphDynamicä¸­çš„DynTreeNodeAux
    """
    def __init__(self, node_id: str, node_type: str, level: int = 0, 
                 parent: Optional[str] = None, children: Optional[Set[str]] = None,
                 update_flag: bool = False, valid_flag: bool = True):
        self.node_id = node_id
        self.node_type = node_type  # 'entity', 'chunk', 'community'
        self.level = level  # å±‚æ¬¡çº§åˆ«
        self.parent = parent  # çˆ¶èŠ‚ç‚¹ID
        self.children = children or set()  # å­èŠ‚ç‚¹IDé›†åˆ
        self.update_flag = update_flag  # æ˜¯å¦éœ€è¦æ›´æ–°
        self.valid_flag = valid_flag  # æ˜¯å¦æœ‰æ•ˆ
        self.last_modified = None  # æœ€åä¿®æ”¹æ—¶é—´
        self.signature = None  # LSHç­¾å


class HKDynamicAux:
    """
    HKGraphTreeçš„åŠ¨æ€è¾…åŠ©ç»“æ„ï¼Œç®¡ç†å¢é‡æ›´æ–°çš„å…ƒä¿¡æ¯
    å‚è€ƒTreeGraphDynamicä¸­çš„DynAuxè®¾è®¡
    """
    def __init__(self, workspace, shape: Tuple[int, int], force: bool = False):
        self.workspace = workspace
        # å¦‚æœworkspaceæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œåˆ™ä¸éœ€è¦ns_clusteringï¼›å¦‚æœæ˜¯workspaceå¯¹è±¡ï¼Œåˆ™åˆ›å»ºns_clustering
        if workspace and hasattr(workspace, 'make_for'):
            self.ns_clustering = workspace.make_for("ns_clustering")
        else:
            self.ns_clustering = None
        
        # æ–‡ä»¶è·¯å¾„å®šä¹‰ - ä½¿ç”¨workspaceæ„å»ºæ­£ç¡®çš„è·¯å¾„
        if workspace:
            # åˆ¤æ–­workspaceæ˜¯è·¯å¾„å­—ç¬¦ä¸²è¿˜æ˜¯workspaceå¯¹è±¡
            if isinstance(workspace, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                base_path = workspace
            elif hasattr(workspace, 'root_path'):
                # å¦‚æœæ˜¯workspaceå¯¹è±¡ï¼Œä½¿ç”¨å…¶root_path
                base_path = workspace.root_path
            else:
                # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨å½“å‰ç›®å½•
                base_path = "."
            
            self.signature_file = os.path.join(base_path, "hk_signatures.pkl")
            self.hyperplane_file = os.path.join(base_path, "hk_hyperplanes.npy")
            self.aux_data_file = os.path.join(base_path, "hk_aux_data.pkl")
        else:
            # å¦‚æœæ²¡æœ‰workspaceï¼Œä½¿ç”¨å½“å‰ç›®å½•ï¼ˆå…¼å®¹æ€§ï¼‰
            self.signature_file = "hk_signatures.pkl"
            self.hyperplane_file = "hk_hyperplanes.npy"
            self.aux_data_file = "hk_aux_data.pkl"
        
        # å¦‚æœå¼ºåˆ¶é‡ç½®ï¼Œåˆ é™¤ç°æœ‰æ–‡ä»¶
        if force:
            for file_path in [self.signature_file, self.hyperplane_file, self.aux_data_file]:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.info(f"Removed existing file: {file_path}")
        
        # æ ¸å¿ƒæ•°æ®ç»“æ„
        self.node_aux = {}  # node_id -> HKNodeAux
        self.signature_map = {}  # node_id -> LSH signature
        self.hyperplanes = self.get_hyperplanes(shape)  # å›ºå®šçš„LSHè¶…å¹³é¢
        self.affected_entities = set()  # éœ€è¦æ›´æ–°çš„èŠ‚ç‚¹é›†åˆ
        self.level_to_nodes = defaultdict(set)  # level -> set of node_ids
        self.node_to_level = {}  # node_id -> level
        
        # å¢é‡æ›´æ–°ç›¸å…³
        self.incremental_mode = False  # æ˜¯å¦å¤„äºå¢é‡æ›´æ–°æ¨¡å¼
        self.base_graph_loaded = False  # åŸºç¡€å›¾æ˜¯å¦å·²åŠ è½½
        self.last_update_timestamp = None
        
        logger.info(f"ğŸ”§ HKDynamicAux initialized with hyperplane shape: {shape}")

    def save_hyperplanes(self, hyperplanes: np.ndarray):
        """ä¿å­˜è¶…å¹³é¢åˆ°æ–‡ä»¶"""
        np.save(self.hyperplane_file, hyperplanes)
        logger.info(f"Saved hyperplanes to {self.hyperplane_file}")

    def load_hyperplanes(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½è¶…å¹³é¢"""
        if os.path.exists(self.hyperplane_file):
            self.hyperplanes = np.load(self.hyperplane_file)
            logger.info(f"âœ… Loaded hyperplanes from {self.hyperplane_file}")
            return True
        return False

    def get_hyperplanes(self, shape: Tuple[int, int], force: bool = False) -> np.ndarray:
        """
        è·å–LSHè¶…å¹³é¢ï¼Œç¡®ä¿ä¸€è‡´æ€§
        """
        if os.path.exists(self.hyperplane_file) and not force:
            hp = np.load(self.hyperplane_file)
            logger.info("âœ… Hyperplane loaded from existing file!")
        else:
            # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯é‡ç°æ€§
            np.random.seed(42)
            hp = np.random.randn(*shape)
            np.save(self.hyperplane_file, hp)
            logger.info("âŒ No existing hyperplane! Generated new hyperplane with fixed seed!")
        return hp

    def save_aux_data(self):
        """ä¿å­˜è¾…åŠ©æ•°æ®åˆ°æ–‡ä»¶"""
        aux_data = {
            'node_aux': {node_id: {
                'node_type': aux.node_type,
                'level': aux.level,
                'parent': aux.parent,
                'children': list(aux.children),
                'update_flag': aux.update_flag,
                'valid_flag': aux.valid_flag,
                'signature': aux.signature
            } for node_id, aux in self.node_aux.items()},
            'signature_map': self.signature_map,
            'affected_entities': list(self.affected_entities),
            'level_to_nodes': {level: list(nodes) for level, nodes in self.level_to_nodes.items()},
            'node_to_level': self.node_to_level,
            'incremental_mode': self.incremental_mode,
            'base_graph_loaded': self.base_graph_loaded
        }
        
        with open(self.aux_data_file, 'wb') as f:
            pickle.dump(aux_data, f)
        logger.info(f"Saved auxiliary data to {self.aux_data_file}")

    def load_aux_data(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½è¾…åŠ©æ•°æ®"""
        if not os.path.exists(self.aux_data_file):
            return False
            
        try:
            with open(self.aux_data_file, 'rb') as f:
                aux_data = pickle.load(f)
            
            # æ¢å¤node_aux
            self.node_aux = {}
            for node_id, data in aux_data.get('node_aux', {}).items():
                aux = HKNodeAux(
                    node_id=node_id,
                    node_type=data['node_type'],
                    level=data['level'],
                    parent=data['parent'],
                    children=set(data['children']),
                    update_flag=data['update_flag'],
                    valid_flag=data['valid_flag']
                )
                aux.signature = data.get('signature')
                self.node_aux[node_id] = aux
            
            # æ¢å¤å…¶ä»–æ•°æ®ç»“æ„
            self.signature_map = aux_data.get('signature_map', {})
            self.affected_entities = set(aux_data.get('affected_entities', []))
            self.level_to_nodes = defaultdict(set)
            for level, nodes in aux_data.get('level_to_nodes', {}).items():
                self.level_to_nodes[int(level)] = set(nodes)
            self.node_to_level = aux_data.get('node_to_level', {})
            self.incremental_mode = aux_data.get('incremental_mode', False)
            self.base_graph_loaded = aux_data.get('base_graph_loaded', False)
            
            logger.info(f"âœ… Loaded auxiliary data with {len(self.node_aux)} nodes")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load auxiliary data: {e}")
            return False

    def add_node_aux(self, node_id: str, node_type: str, level: int = 0, 
                     parent: Optional[str] = None, children: Optional[Set[str]] = None):
        """æ·»åŠ èŠ‚ç‚¹è¾…åŠ©ä¿¡æ¯"""
        aux = HKNodeAux(
            node_id=node_id,
            node_type=node_type,
            level=level,
            parent=parent,
            children=children or set(),
            update_flag=True,  # æ–°èŠ‚ç‚¹é»˜è®¤éœ€è¦æ›´æ–°
            valid_flag=True
        )
        self.node_aux[node_id] = aux
        self.level_to_nodes[level].add(node_id)
        self.node_to_level[node_id] = level
        
        # æ ‡è®°ä¸ºå—å½±å“çš„å®ä½“
        self.affected_entities.add(node_id)
        
        #logger.debug(f"Added node aux for {node_id} at level {level}")

    def update_node_level(self, node_id: str, new_level: int):
        """æ›´æ–°èŠ‚ç‚¹å±‚çº§"""
        if node_id in self.node_aux:
            old_level = self.node_aux[node_id].level
            if old_level != new_level:
                # ä»æ—§å±‚çº§ç§»é™¤
                self.level_to_nodes[old_level].discard(node_id)
                # æ·»åŠ åˆ°æ–°å±‚çº§
                self.level_to_nodes[new_level].add(node_id)
                self.node_to_level[node_id] = new_level
                self.node_aux[node_id].level = new_level
                self.node_aux[node_id].update_flag = True
                self.affected_entities.add(node_id)

    def set_parent_child_relationship(self, parent_id: str, child_id: str):
        """è®¾ç½®çˆ¶å­å…³ç³»"""
        if parent_id in self.node_aux and child_id in self.node_aux:
            # è®¾ç½®çˆ¶å­å…³ç³»
            self.node_aux[child_id].parent = parent_id
            self.node_aux[parent_id].children.add(child_id)
            
            # æ ‡è®°ä¸ºå—å½±å“
            self.affected_entities.add(parent_id)
            self.affected_entities.add(child_id)

    def mark_node_invalid(self, node_id: str):
        """æ ‡è®°èŠ‚ç‚¹ä¸ºæ— æ•ˆ"""
        if node_id in self.node_aux:
            self.node_aux[node_id].valid_flag = False
            self.affected_entities.add(node_id)
            
            # ä»å±‚çº§æ˜ å°„ä¸­ç§»é™¤
            level = self.node_aux[node_id].level
            self.level_to_nodes[level].discard(node_id)
            
            #logger.debug(f"Marked node {node_id} as invalid")

    def get_valid_nodes_at_level(self, level: int) -> List[str]:
        """è·å–æŒ‡å®šå±‚çº§çš„æœ‰æ•ˆèŠ‚ç‚¹"""
        nodes = self.level_to_nodes.get(level, set())
        return [node_id for node_id in nodes 
                if node_id in self.node_aux and self.node_aux[node_id].valid_flag]

    def get_affected_nodes_at_level(self, level: int) -> List[str]:
        """è·å–æŒ‡å®šå±‚çº§éœ€è¦æ›´æ–°çš„èŠ‚ç‚¹"""
        nodes = self.get_valid_nodes_at_level(level)
        return [node_id for node_id in nodes 
                if node_id in self.affected_entities]

    def clear_update_flags(self):
        """æ¸…é™¤æ‰€æœ‰æ›´æ–°æ ‡å¿—"""
        for aux in self.node_aux.values():
            aux.update_flag = False
        self.affected_entities.clear()

    def compute_signature(self, embedding: np.ndarray) -> int:
        """è®¡ç®—èŠ‚ç‚¹åµŒå…¥çš„LSHç­¾å"""
        if self.hyperplanes is None:
            raise ValueError("Hyperplanes not initialized")
        
        projections = np.dot(embedding, self.hyperplanes.T)
        binary_hash = (projections > 0).astype(int)
        return int(''.join(map(str, binary_hash)), 2)

    def update_node_signature(self, node_id: str, embedding: np.ndarray):
        """æ›´æ–°èŠ‚ç‚¹çš„LSHç­¾å"""
        signature = self.compute_signature(embedding)
        self.signature_map[node_id] = signature
        if node_id in self.node_aux:
            self.node_aux[node_id].signature = signature
        return signature

    def mark_node_affected(self, node_id: str):
        """æ ‡è®°èŠ‚ç‚¹ä¸ºå—å½±å“"""
        self.affected_entities.add(node_id)
        if node_id in self.node_aux:
            self.node_aux[node_id].update_flag = True

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¾…åŠ©ç»“æ„çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_nodes': len(self.node_aux),
            'valid_nodes': sum(1 for aux in self.node_aux.values() if aux.valid_flag),
            'affected_nodes': len(self.affected_entities),
            'level_distribution': {level: len(nodes) for level, nodes in self.level_to_nodes.items()},
            'node_type_distribution': defaultdict(int)
        }
        
        for aux in self.node_aux.values():
            if aux.valid_flag:
                stats['node_type_distribution'][aux.node_type] += 1
        
        return stats


class HKGraphTreeDynamic(HKGraphTree):
    """
    HKGraphTreeçš„åŠ¨æ€å¢é‡æ›´æ–°ç‰ˆæœ¬
    
    é›†æˆäº†EraRAG TreeGraphDynamicçš„å¢é‡æ›´æ–°æœºåˆ¶ï¼š
    1. å›ºå®šLSHè¶…å¹³é¢ç¡®ä¿ç­¾åä¸€è‡´æ€§
    2. ç»†ç²’åº¦çš„å—å½±å“èŠ‚ç‚¹è¿½è¸ª
    3. å±‚æ¬¡åŒ–çš„å±€éƒ¨é‡æ„
    4. é«˜æ•ˆçš„å¢é‡åµŒå…¥æ›´æ–°
    """
    
    def __init__(self, config, embed_config, llm, encoder, **kwargs):
        super().__init__(config, embed_config, llm, encoder, **kwargs)
        
        # å¢é‡æ›´æ–°é…ç½®
        self.enable_incremental_update = getattr(config, 'enable_incremental_update', True)
        self.incremental_batch_size = getattr(config, 'incremental_batch_size', 10)
        self.max_affected_ratio = getattr(config, 'max_affected_ratio', 0.5)  # æœ€å¤§å—å½±å“èŠ‚ç‚¹æ¯”ä¾‹
        self.enable_cross_chunk_connections = getattr(config, 'enable_cross_chunk_connections', True)  # å¯ç”¨æ–°æ—§chunkè¿æ¥
        
        # åˆå§‹åŒ–åŠ¨æ€è¾…åŠ©ç»“æ„
        hyperplane_shape = (self.lsh_num_hyperplanes, self.cleora_dim)
        workspace = getattr(config, 'faiss_index_path', './faiss_index_temp/')
        self.aux = HKDynamicAux(workspace, hyperplane_shape, force=False)
        
        # å¢é‡æ›´æ–°çŠ¶æ€ç®¡ç†
        self.incremental_mode = False
        self.base_hierarchy_built = False
        
        logger.info(f"ğŸš€ HKGraphTreeDynamic initialized with incremental update enabled: {self.enable_incremental_update}")

    async def _load_graph(self, force: bool = False) -> bool:
        """
        é‡å†™åŠ è½½æ–¹æ³•ï¼Œæ”¯æŒå¢é‡æ›´æ–°æ¨¡å¼
        """
        # é¦–å…ˆå°è¯•åŠ è½½åŸºç¡€å›¾å’Œå±‚æ¬¡ç»“æ„
        base_loaded = await super()._load_graph(force)
        
        if base_loaded:
            # å°è¯•åŠ è½½è¾…åŠ©æ•°æ®
            aux_loaded = self.aux.load_aux_data()
            if aux_loaded:
                self.aux.base_graph_loaded = True
                self.base_hierarchy_built = True
                logger.info("âœ… Successfully loaded base graph and auxiliary data for incremental updates")
                return True
            else:
                logger.warning("âš ï¸ Base graph loaded but auxiliary data missing - will need to rebuild aux structure")
                # å¦‚æœåŸºç¡€å›¾å­˜åœ¨ä½†è¾…åŠ©æ•°æ®ç¼ºå¤±ï¼Œéœ€è¦é‡æ–°æ„å»ºè¾…åŠ©ç»“æ„
                await self._rebuild_aux_structure()
                return True
        
        return False

    async def _rebuild_aux_structure(self):
        """
        ä»ç°æœ‰å›¾ç»“æ„é‡å»ºè¾…åŠ©æ•°æ®ç»“æ„
        """
        logger.info("ğŸ”§ Rebuilding auxiliary structure from existing graph...")
        
        # æ¸…ç†ç°æœ‰è¾…åŠ©æ•°æ®
        self.aux.node_aux.clear()
        self.aux.signature_map.clear()
        self.aux.affected_entities.clear()
        self.aux.level_to_nodes.clear()
        self.aux.node_to_level.clear()
        
        # é‡å»ºåŸºç¡€èŠ‚ç‚¹çš„è¾…åŠ©ä¿¡æ¯
        all_nodes = await self._graph.get_nodes()
        for node_id in all_nodes:
            node_data = await self._graph.get_node(node_id)
            if node_data:
                # ç¡®å®šèŠ‚ç‚¹ç±»å‹
                if node_id.startswith('CHUNK_'):
                    node_type = 'chunk'
                elif node_id.startswith('COMMUNITY_'):
                    node_type = 'community'
                else:
                    node_type = 'entity'
                
                # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
                self.aux.add_node_aux(node_id, node_type, level=0)
                
                # å¦‚æœæœ‰åµŒå…¥ï¼Œè®¡ç®—ç­¾å
                if node_id in self.node_embeddings:
                    embedding = self.node_embeddings[node_id]
                    self.aux.update_node_signature(node_id, embedding)
        
        # é‡å»ºå±‚æ¬¡ç»“æ„çš„è¾…åŠ©ä¿¡æ¯
        if hasattr(self, 'hierarchy_levels'):
            for level, communities in self.hierarchy_levels.items():
                for community_data in communities:
                    # ä»community_dataå­—å…¸ä¸­æå–community_id
                    if isinstance(community_data, dict):
                        community_id = community_data.get('id')
                    else:
                        # å…¼å®¹å¯èƒ½çš„å­—ç¬¦ä¸²æ ¼å¼
                        community_id = community_data
                    
                    if community_id and community_id not in self.aux.node_aux:
                        self.aux.add_node_aux(community_id, 'community', level=int(level)+1)
                    elif community_id:
                        self.aux.update_node_level(community_id, int(level)+1)
                    
                    # å»ºç«‹çˆ¶å­å…³ç³»
                    if community_id:
                        children = self.community_children.get(community_id, [])
                        for child_id in children:
                            if child_id in self.aux.node_aux:
                                self.aux.set_parent_child_relationship(community_id, child_id)
        
        # ä¿å­˜é‡å»ºçš„è¾…åŠ©æ•°æ®
        self.aux.save_aux_data()
        self.aux.base_graph_loaded = True
        self.base_hierarchy_built = True
        
        stats = self.aux.get_statistics()
        logger.info(f"âœ… Rebuilt auxiliary structure: {stats}")

    async def _build_graph(self, chunk_list: List[Any]):
        """
        é‡å†™å›¾æ„å»ºæ–¹æ³•ï¼Œç¡®ä¿åœ¨åˆå§‹æ„å»ºæ—¶ä¹Ÿåˆ›å»ºè¾…åŠ©æ•°æ®ç»“æ„
        """
        # è°ƒç”¨çˆ¶ç±»çš„æ„å»ºæ–¹æ³•
        await super()._build_graph(chunk_list)
        
        # å¦‚æœæ˜¯åˆå§‹æ„å»ºï¼ˆéå¢é‡æ¨¡å¼ï¼‰ï¼Œåˆ›å»ºè¾…åŠ©æ•°æ®ç»“æ„
        if not self.incremental_mode and self.enable_incremental_update:
            logger.info("ğŸ”§ Creating auxiliary data structure for future incremental updates")
            await self._create_initial_aux_structure()
    
    async def _create_initial_aux_structure(self):
        """
        ä¸ºåˆå§‹æ„å»ºçš„å›¾åˆ›å»ºè¾…åŠ©æ•°æ®ç»“æ„
        """
        logger.info("ğŸ› ï¸ Creating initial auxiliary structure")
        
        # æ¸…ç†ç°æœ‰è¾…åŠ©æ•°æ®
        self.aux.node_aux.clear()
        self.aux.signature_map.clear()
        self.aux.affected_entities.clear()
        self.aux.level_to_nodes.clear()
        self.aux.node_to_level.clear()
        
        # ä¸ºæ‰€æœ‰åŸºç¡€èŠ‚ç‚¹åˆ›å»ºè¾…åŠ©ä¿¡æ¯
        all_nodes = await self._graph.get_nodes()
        for node_id in all_nodes:
            node_data = await self._graph.get_node(node_id)
            if node_data:
                # ç¡®å®šèŠ‚ç‚¹ç±»å‹
                if node_id.startswith('CHUNK_'):
                    node_type = 'chunk'
                elif node_id.startswith('COMMUNITY_'):
                    node_type = 'community'
                else:
                    node_type = 'entity'
                
                # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
                self.aux.add_node_aux(node_id, node_type, level=0)
                
                # å¦‚æœæœ‰åµŒå…¥ï¼Œè®¡ç®—ç­¾å
                if node_id in self.node_embeddings:
                    embedding = self.node_embeddings[node_id]
                    self.aux.update_node_signature(node_id, embedding)
        
        # ä¸ºå±‚æ¬¡ç»“æ„åˆ›å»ºè¾…åŠ©ä¿¡æ¯
        if hasattr(self, 'hierarchy_levels'):
            for level, communities in self.hierarchy_levels.items():
                for community_data in communities:
                    # ä»community_dataå­—å…¸ä¸­æå–community_id
                    if isinstance(community_data, dict):
                        community_id = community_data.get('id')
                    else:
                        community_id = community_data
                    
                    if community_id:
                        # æ›´æ–°èŠ‚ç‚¹å±‚çº§ä¿¡æ¯
                        if community_id in self.aux.node_aux:
                            self.aux.update_node_level(community_id, int(level)+1)
                        else:
                            self.aux.add_node_aux(community_id, 'community', level=int(level)+1)
                        
                        # å»ºç«‹çˆ¶å­å…³ç³»
                        children = self.community_children.get(community_id, [])
                        for child_id in children:
                            if child_id in self.aux.node_aux:
                                self.aux.set_parent_child_relationship(community_id, child_id)
        
        # æ¸…é™¤æ›´æ–°æ ‡å¿—ï¼ˆåˆå§‹çŠ¶æ€ä¸‹æ‰€æœ‰èŠ‚ç‚¹éƒ½æ˜¯"æ–°"çš„ï¼Œä½†ä¸éœ€è¦æ›´æ–°æ ‡å¿—ï¼‰
        self.aux.clear_update_flags()
        
        # ä¿å­˜è¾…åŠ©æ•°æ®
        self.aux.save_aux_data()
        self.aux.base_graph_loaded = True
        self.base_hierarchy_built = True
        
        stats = self.aux.get_statistics()
        logger.info(f"âœ… Created initial auxiliary structure: {stats}")

    async def insert_incremental(self, new_chunk_list: List[Any]) -> bool:
        """
        å¢é‡æ’å…¥æ–°çš„æ–‡æ¡£å—
        
        Args:
            new_chunk_list: æ–°çš„æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[(chunk_key, TextChunk), ...]
            
        Returns:
            bool: æ’å…¥æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_incremental_update:
            logger.warning("Incremental update is disabled, falling back to full rebuild")
            return await self._build_graph(new_chunk_list)
        
        if not self.base_hierarchy_built:
            logger.info("Base hierarchy not built, performing initial build...")
            return await self._build_graph(new_chunk_list)
        
        logger.info(f"ğŸš€ Starting incremental update with {len(new_chunk_list)} new chunks")
        
        try:
            # è®¾ç½®å¢é‡æ¨¡å¼
            self.incremental_mode = True
            self.aux.incremental_mode = True
            
            # Step 1: å¤„ç†æ–°çš„æ–‡æ¡£å—ï¼Œæ„å»ºåŸºç¡€å›¾éƒ¨åˆ†
            await self._process_incremental_chunks(new_chunk_list)
            
            # Step 2: æ›´æ–°CleoraåµŒå…¥
            await self._update_cleora_embeddings_incremental()
            
            # Step 3: æ‰§è¡Œå¢é‡å±‚æ¬¡åŒ–èšç±»
            await self._update_hierarchy_incremental()
            
            # Step 4: æ›´æ–°FAISSç´¢å¼•
            await self._update_faiss_indexes_incremental()
            
            # Step 5: ä¿å­˜æ›´æ–°åçš„æ•°æ®
            await self._save_incremental_updates()
            
            # æ¸…ç†æ›´æ–°æ ‡å¿—
            self.aux.clear_update_flags()
            
            stats = self.aux.get_statistics()
            logger.info(f"âœ… Incremental update completed successfully: {stats}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Incremental update failed: {e}")
            self.incremental_mode = False
            self.aux.incremental_mode = False
            raise
        
        finally:
            self.incremental_mode = False
            self.aux.incremental_mode = False

    async def _process_incremental_chunks(self, new_chunk_list: List[Any]):
        """
        å¤„ç†æ–°çš„æ–‡æ¡£å—ï¼Œå¢é‡æ„å»ºåŸºç¡€å›¾
        """
        logger.info(f"ğŸ“ Processing {len(new_chunk_list)} new chunks for incremental update")
        
        # Step 1: å¯¹æ–°chunksè¿›è¡Œå®ä½“å…³ç³»æŠ½å–
        er_results = []
        passage_results = []
        
        logger.info("ğŸ› ï¸ Extracting entities and relationships from new chunks")
        
        # ä½¿ç”¨å¹¶å‘æ§åˆ¶å¤„ç†chunk
        er_results, passage_results = await self._process_chunks_with_concurrency_control(new_chunk_list)
        
        # Step 2: å¢é‡æ„å»ºæ··åˆå›¾
        await self._build_incremental_hybrid_graph(er_results, passage_results, new_chunk_list)

    async def _process_chunks_with_concurrency_control(self, chunk_list: List[Any]) -> Tuple[List[Dict], List[Dict]]:
        """
        ä½¿ç”¨å¹¶å‘æ§åˆ¶å¤„ç†chunksçš„å®ä½“å…³ç³»æŠ½å–
        """
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _process_single_chunk(chunk_data):
            async with semaphore:
                try:
                    # ä»å­—å…¸ä¸­æå–chunkä¿¡æ¯
                    if isinstance(chunk_data, dict):
                        chunk_content = chunk_data.get('content', '')
                        # ä½¿ç”¨å†…å®¹hashä½œä¸ºchunk_keyï¼Œä¸å­˜å‚¨ç³»ç»Ÿä¿æŒä¸€è‡´
                        from Core.Common.Utils import mdhash_id
                        chunk_key = mdhash_id(chunk_content.strip(), prefix="doc-")
                    else:
                        # å…¼å®¹åŸæœ‰çš„å…ƒç»„æ ¼å¼
                        chunk_key, chunk_info = chunk_data
                        chunk_content = chunk_info.content
                    
                    # å®ä½“å…³ç³»æŠ½å–
                    if self.extract_two_step:
                        entities = await self._named_entity_recognition(chunk_content)
                        triples = await self._openie_post_ner_extract(chunk_content, entities)
                    else:
                        content = await self._kg_agent(chunk_content)
                        entities, triples = await self._parse_kg_content(content)
                    
                    entities_dict, relationships_dict = await self._build_graph_from_tuples(entities, triples, chunk_key)
                    er_result = {
                        'chunk_key': chunk_key,
                        'entities': entities_dict,
                        'relationships': relationships_dict
                    }
                    
                    # ç»´åŸºç™¾ç§‘å®ä½“é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.use_wat_linking:
                        wiki_entities = await self._extract_wiki_entities(chunk_content)
                        passage_result = {
                            'chunk_key': chunk_key,
                            'wiki_entities': wiki_entities
                        }
                    else:
                        passage_result = {
                            'chunk_key': chunk_key,
                            'wiki_entities': entities_dict
                        }
                    
                    return er_result, passage_result
                    
                except Exception as e:
                    logger.error(f"Failed to process chunk {chunk_key}: {e}")
                    return None, None
        
        # æ‰§è¡Œå¹¶å‘å¤„ç†
        tasks = [_process_single_chunk(chunk_data) for chunk_data in chunk_list]
        logger.info(f"ğŸ”§ Processing {len(tasks)} chunks with max concurrency {max_concurrent}")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†ç¦»ç»“æœ
        er_results = []
        passage_results = []
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Chunk processing failed with exception: {result}")
                continue
            elif result and len(result) == 2:
                er_result, passage_result = result
                if er_result and passage_result:
                    er_results.append(er_result)
                    passage_results.append(passage_result)
        
        logger.info(f"âœ… Successfully processed {len(er_results)} chunks")
        return er_results, passage_results

    async def _build_incremental_hybrid_graph(self, er_results: List[Dict], passage_results: List[Dict], chunk_list: List[Any]):
        """
        å¢é‡æ„å»ºæ··åˆå›¾ï¼Œåªæ·»åŠ æ–°çš„èŠ‚ç‚¹å’Œè¾¹
        """
        logger.info("ğŸ› ï¸ Building incremental hybrid graph")
        
        all_entities = defaultdict(list)
        all_relationships = defaultdict(list)
        chunk_entities_map = defaultdict(set)
        entity_chunks_map = defaultdict(set)
        wiki_entities_map = defaultdict(list)
        
        # å¤„ç†ERç»“æœ
        for er_result in er_results:
            chunk_key = er_result['chunk_key']
            entities = er_result['entities']
            relationships = er_result['relationships']
            
            for entity_name, entity_list in entities.items():
                all_entities[entity_name].extend(entity_list)
                chunk_entities_map[chunk_key].add(entity_name)
                entity_chunks_map[entity_name].add(chunk_key)
            
            for rel_key, rel_list in relationships.items():
                all_relationships[rel_key].extend(rel_list)
        
        # å¤„ç†passageç»“æœ
        for passage_result in passage_results:
            chunk_key = passage_result['chunk_key']
            wiki_entities = passage_result['wiki_entities']
            
            for wiki_entity, _ in wiki_entities.items():
                wiki_entities_map[wiki_entity].append(chunk_key)
        
        # åˆ›å»ºæ–°çš„chunkèŠ‚ç‚¹
        new_chunk_nodes = defaultdict(list)
        for chunk_data in chunk_list:
            # ä»å­—å…¸ä¸­æå–chunkä¿¡æ¯
            if isinstance(chunk_data, dict):
                chunk_content = chunk_data.get('content', '')
                # ä½¿ç”¨å†…å®¹hashä½œä¸ºchunk_keyï¼Œä¸å­˜å‚¨ç³»ç»Ÿä¿æŒä¸€è‡´
                from Core.Common.Utils import mdhash_id
                chunk_key = mdhash_id(chunk_content.strip(), prefix="doc-")
            else:
                # å…¼å®¹åŸæœ‰çš„å…ƒç»„æ ¼å¼
                chunk_key, chunk_info = chunk_data
                chunk_content = chunk_info.content
            
            chunk_node_id = f"CHUNK_{chunk_key}"
            chunk_entity = HK_Node(
                entity_name=chunk_node_id,
                entity_type="CHUNK",
                description=chunk_content,
                source_id=chunk_key
            )
            new_chunk_nodes[chunk_node_id].append(chunk_entity)
            
            # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
            self.aux.add_node_aux(chunk_node_id, 'chunk', level=0)
        
        # åªæ·»åŠ æ–°çš„å®ä½“èŠ‚ç‚¹ï¼ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼‰
        new_entity_nodes = defaultdict(list)
        for entity_name, entity_list in all_entities.items():
            # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
            if not await self._graph.has_node(entity_name):
                new_entity_nodes[entity_name] = entity_list
                # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
                self.aux.add_node_aux(entity_name, 'entity', level=0)
            else:
                # å¦‚æœå®ä½“å·²å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå—å½±å“ï¼ˆå¯èƒ½éœ€è¦æ›´æ–°è¿æ¥ï¼‰
                self.aux.affected_entities.add(entity_name)
        
        # åˆ›å»ºæ–°çš„å…³ç³»
        new_entity_chunk_relationships = defaultdict(list)
        new_chunk_chunk_relationships = defaultdict(list)
        
        # å®ä½“-chunkè¿æ¥
        for entity_name, chunk_keys in entity_chunks_map.items():
            for chunk_key in chunk_keys:
                rel_key = (entity_name, f"CHUNK_{chunk_key}")
                relationship = Relationship(
                    src_id=entity_name,
                    tgt_id=f"CHUNK_{chunk_key}",
                    relation_name="BELONGS_TO",
                    description=f"Entity {entity_name} belongs to chunk {chunk_key}",
                    source_id=f"{entity_name}_{chunk_key}",
                    weight=1.0
                )
                new_entity_chunk_relationships[rel_key].append(relationship)
        
        # Chunk-chunkè¿æ¥ï¼ˆåŸºäºå…±äº«å®ä½“ï¼‰- åŒ…æ‹¬æ–°chunkä¸ç°æœ‰chunkçš„å®Œæ•´è¿æ¥
        chunk_pair_shared_entities = defaultdict(list)
        
        # Step 1: æ”¶é›†æ‰€æœ‰æ¶‰åŠçš„å®ä½“
        all_entities_in_new_chunks = set(wiki_entities_map.keys())
        logger.info(f"ğŸ” Processing {len(all_entities_in_new_chunks)} entities for chunk-chunk connections")
        
        # Step 2: ä¸ºæ¯ä¸ªå®ä½“æŸ¥è¯¢ç°æœ‰å›¾ä¸­çš„ç›¸å…³chunkï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        entity_to_existing_chunks = {}
        if self.enable_cross_chunk_connections:
            logger.info("ğŸ”— Cross-chunk connections enabled, querying existing chunks")
            entity_to_existing_chunks = await self._get_existing_chunks_for_entities_batch(all_entities_in_new_chunks)
        else:
            logger.info("ğŸš« Cross-chunk connections disabled, skipping existing chunk queries")
        
        logger.info(f"ğŸ“Š Found {len(entity_to_existing_chunks)} entities with existing chunk connections")
        
        # Step 3: å»ºç«‹å®Œæ•´çš„chunk-chunkè¿æ¥ï¼ˆæ–°-æ–°ã€æ–°-æ—§ï¼‰
        for wiki_entity, new_chunk_keys in wiki_entities_map.items():
            # è·å–è¯¥å®ä½“ç›¸å…³çš„ç°æœ‰chunk
            existing_chunk_keys = entity_to_existing_chunks.get(wiki_entity, [])
            
            # åˆå¹¶æ–°æ—§chunkåˆ—è¡¨
            all_chunk_keys = list(set(new_chunk_keys + existing_chunk_keys))
            
            if len(all_chunk_keys) < 2:
                continue
            
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„chunkå¯¹
            for chunk1, chunk2 in combinations(all_chunk_keys, 2):
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ˜¯æ–°chunkï¼ˆé¿å…é‡å¤å¤„ç†æ—§chunkä¹‹é—´çš„è¿æ¥ï¼‰
                if chunk1 in new_chunk_keys or chunk2 in new_chunk_keys:
                    chunk_pair = tuple(sorted([chunk1, chunk2]))
                    chunk_pair_shared_entities[chunk_pair].append(wiki_entity)
        
        logger.info(f"ğŸ”— Generated {len(chunk_pair_shared_entities)} potential chunk-chunk connections")
        
        # Step 4: åº”ç”¨å…±äº«å®ä½“é˜ˆå€¼å¹¶åˆ›å»ºè¿æ¥
        shared_entity_threshold = getattr(self.config, 'shared_entity_threshold', 2)
        new_new_connections = 0  # æ–°chunkä¸æ–°chunkçš„è¿æ¥
        new_old_connections = 0  # æ–°chunkä¸æ—§chunkçš„è¿æ¥
        
        for (chunk1, chunk2), shared_entities in chunk_pair_shared_entities.items():
            if len(shared_entities) < shared_entity_threshold:
                continue
            
            # ç»Ÿè®¡è¿æ¥ç±»å‹
            chunk1_is_new = any(chunk1 in chunk_keys for chunk_keys in wiki_entities_map.values())
            chunk2_is_new = any(chunk2 in chunk_keys for chunk_keys in wiki_entities_map.values())
            
            if chunk1_is_new and chunk2_is_new:
                new_new_connections += 1
            elif chunk1_is_new or chunk2_is_new:
                new_old_connections += 1
            
            rel_key = tuple(sorted([f"CHUNK_{chunk1}", f"CHUNK_{chunk2}"]))
            relationship = Relationship(
                src_id=rel_key[0],
                tgt_id=rel_key[1],
                relation_name="SHARED_ENTITY",
                description=f"Chunks connected through shared entities: {', '.join(shared_entities)}",
                source_id=GRAPH_FIELD_SEP.join([chunk1, chunk2] + shared_entities),
                weight=float(len(shared_entities))
            )
            new_chunk_chunk_relationships[rel_key].append(relationship)
        
        logger.info(f"ğŸ“Š Chunk-chunk connection statistics:")
        logger.info(f"   ğŸ”— New-New connections: {new_new_connections}")
        logger.info(f"   ğŸ”— New-Old connections: {new_old_connections}")
        logger.info(f"   ğŸ”— Total connections: {len(new_chunk_chunk_relationships)}")
        logger.info(f"   ğŸ“ Shared entity threshold: {shared_entity_threshold}")
        
        # å°†æ–°èŠ‚ç‚¹å’Œè¾¹æ·»åŠ åˆ°å›¾ä¸­
        logger.info("ğŸ› ï¸ Adding new nodes and edges to graph")
        
        # æ·»åŠ æ–°èŠ‚ç‚¹ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        all_new_nodes = {**new_entity_nodes, **new_chunk_nodes}
        if all_new_nodes:
            await self._add_nodes_with_concurrency_control(all_new_nodes)
        
        # æ·»åŠ æ–°è¾¹ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        all_new_edges = {**all_relationships, **new_entity_chunk_relationships, **new_chunk_chunk_relationships}
        if all_new_edges:
            await self._add_edges_with_concurrency_control(all_new_edges)
        
        logger.info(f"âœ… Added {len(all_new_nodes)} new nodes and {len(all_new_edges)} new edges to graph")

    async def _get_existing_chunks_for_entity(self, entity_name: str) -> List[str]:
        """
        æŸ¥è¯¢ç°æœ‰å›¾ä¸­åŒ…å«æŒ‡å®šå®ä½“çš„chunkèŠ‚ç‚¹
        
        Args:
            entity_name: å®ä½“åç§°
            
        Returns:
            List[str]: åŒ…å«è¯¥å®ä½“çš„ç°æœ‰chunkçš„keyåˆ—è¡¨ï¼ˆä¸åŒ…å«CHUNK_å‰ç¼€ï¼‰
        """
        try:
            # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨äºå›¾ä¸­
            if not await self._graph.has_node(entity_name):
                return []
            
            # é€šè¿‡å®ä½“èŠ‚ç‚¹æŸ¥æ‰¾å…¶é‚»å±…chunkèŠ‚ç‚¹
            neighbors = await self._graph.neighbors(entity_name)
            existing_chunks = []
            
            for neighbor in neighbors:
                if neighbor.startswith('CHUNK_'):
                    # æå–chunk keyï¼ˆç§»é™¤CHUNK_å‰ç¼€ï¼‰
                    chunk_key = neighbor.replace('CHUNK_', '')
                    existing_chunks.append(chunk_key)
            
            logger.debug(f"Entity '{entity_name}' connected to {len(existing_chunks)} existing chunks")
            return existing_chunks
            
        except Exception as e:
            logger.warning(f"Failed to get existing chunks for entity '{entity_name}': {e}")
            return []

    async def _get_existing_chunks_for_entities_batch(self, entity_names: set) -> Dict[str, List[str]]:
        """
        æ‰¹é‡æŸ¥è¯¢å¤šä¸ªå®ä½“åœ¨ç°æœ‰å›¾ä¸­çš„ç›¸å…³chunkèŠ‚ç‚¹ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        
        Args:
            entity_names: å®ä½“åç§°é›†åˆ
            
        Returns:
            Dict[str, List[str]]: å®ä½“åç§° -> chunk keyåˆ—è¡¨çš„æ˜ å°„
        """
        if not entity_names:
            return {}
        
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _get_single_entity_chunks(entity_name):
            async with semaphore:
                existing_chunks = await self._get_existing_chunks_for_entity(entity_name)
                return entity_name, existing_chunks
        
        # æ‰§è¡Œå¹¶å‘æŸ¥è¯¢
        tasks = [_get_single_entity_chunks(entity_name) for entity_name in entity_names]
        logger.info(f"ğŸ”§ Querying existing chunks for {len(tasks)} entities with max concurrency {max_concurrent}")
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        entity_to_existing_chunks = {}
        successful_queries = 0
        
        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"Failed to query entity chunks: {result}")
                continue
            
            entity_name, existing_chunks = result
            if existing_chunks:
                entity_to_existing_chunks[entity_name] = existing_chunks
                successful_queries += 1
                #logger.debug(f"Entity '{entity_name}' found in {len(existing_chunks)} existing chunks")
        
        logger.info(f"âœ… Successfully queried {successful_queries}/{len(entity_names)} entities for existing chunk connections")
        return entity_to_existing_chunks

    async def _add_nodes_with_concurrency_control(self, nodes_dict: Dict):
        """
        ä½¿ç”¨å¹¶å‘æ§åˆ¶æ·»åŠ èŠ‚ç‚¹
        """
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _add_single_node(key, value):
            async with semaphore:
                await self._merge_nodes_then_upsert(key, value)
        
        tasks = [_add_single_node(k, v) for k, v in nodes_dict.items()]
        logger.info(f"ğŸ”§ Adding {len(tasks)} nodes with max concurrency {max_concurrent}")
        await asyncio.gather(*tasks)

    async def _add_edges_with_concurrency_control(self, edges_dict: Dict):
        """
        ä½¿ç”¨å¹¶å‘æ§åˆ¶æ·»åŠ è¾¹
        """
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _add_single_edge(key, value):
            async with semaphore:
                if isinstance(key, tuple) and len(key) == 2:
                    await self._merge_edges_then_upsert(key[0], key[1], value)
                else:
                    logger.warning(f"Invalid edge key format: {key}")
        
        tasks = [_add_single_edge(k, v) for k, v in edges_dict.items()]
        logger.info(f"ğŸ”§ Adding {len(tasks)} edges with max concurrency {max_concurrent}")
        await asyncio.gather(*tasks)

    async def _update_cleora_embeddings_incremental(self):
        """
        å¢é‡æ›´æ–°CleoraåµŒå…¥ï¼Œåªå¤„ç†å—å½±å“çš„èŠ‚ç‚¹
        ä½¿ç”¨ä¸åŸç‰ˆHKGraphTreeç›¸åŒçš„Cleoraç®—æ³•
        """
        logger.info("ğŸ”„ Updating Cleora embeddings incrementally")
        
        # è·å–æ‰€æœ‰å—å½±å“çš„èŠ‚ç‚¹
        affected_nodes = list(self.aux.affected_entities)
        if not affected_nodes:
            logger.info("No affected nodes found, skipping embedding update")
            return
        
        logger.info(f"Updating embeddings for {len(affected_nodes)} affected nodes")
        
        # Step 1: ä¸ºæ–°èŠ‚ç‚¹ç”Ÿæˆåˆå§‹æ–‡æœ¬åµŒå…¥
        new_nodes = []
        for node_id in affected_nodes:
            if node_id not in self.node_text_embeddings:
                try:
                    if node_id.startswith('CHUNK_'):
                        # ChunkèŠ‚ç‚¹ï¼šä½¿ç”¨chunkå†…å®¹
                        chunk_key = node_id.replace('CHUNK_', '')
                        node_data = await self._graph.get_node(node_id)
                        if node_data and 'description' in node_data:
                            text_embedding = await self._embed_text(node_data['description'])
                            self.node_text_embeddings[node_id] = np.array(text_embedding)
                    else:
                        # å®ä½“èŠ‚ç‚¹ï¼šä½¿ç”¨å®ä½“åç§°å’Œæè¿°
                        node_data = await self._graph.get_node(node_id)
                        if node_data:
                            text_content = node_data.get('entity_name', node_id)
                            if 'description' in node_data:
                                text_content += ": " + node_data['description']
                            text_embedding = await self._embed_text(text_content)
                            self.node_text_embeddings[node_id] = np.array(text_embedding)
                    
                    # æ–°èŠ‚ç‚¹åˆå§‹ä½¿ç”¨æ–‡æœ¬åµŒå…¥ä½œä¸ºèŠ‚ç‚¹åµŒå…¥
                    if node_id in self.node_text_embeddings:
                        self.node_embeddings[node_id] = self.node_text_embeddings[node_id].copy()
                        new_nodes.append(node_id)
                        
                except Exception as e:
                    logger.warning(f"Failed to generate text embedding for {node_id}: {e}")
        
        logger.info(f"Generated text embeddings for {len(new_nodes)} new nodes")
        
        # Step 2: è·å–æ‰€æœ‰èŠ‚ç‚¹çš„é‚»æ¥ä¿¡æ¯ï¼ˆåªéœ€è¦å—å½±å“èŠ‚ç‚¹çš„é‚»æ¥ä¿¡æ¯ï¼‰
        adj_list = {}
        for node_id in affected_nodes:
            try:
                neighbors = list(await self._graph.neighbors(node_id))
                adj_list[node_id] = neighbors
            except Exception as e:
                logger.warning(f"Failed to get neighbors for {node_id}: {e}")
                adj_list[node_id] = []
        
        # Step 3: æ‰§è¡ŒCleoraè¿­ä»£ï¼ˆä¸åŸç‰ˆç®—æ³•ç›¸åŒï¼‰
        logger.info(f"Running Cleora iterations (iterations={self.cleora_iterations})")
        
        for iteration in range(self.cleora_iterations):
            logger.debug(f"Cleora iteration {iteration + 1}/{self.cleora_iterations}")
            
            # åˆ›å»ºä¸´æ—¶å­˜å‚¨ç”¨äºæ›´æ–°åçš„åµŒå…¥
            updated_embeddings = {}
            
            for node_id in affected_nodes:
                if node_id not in self.node_embeddings:
                    continue
                    
                neighbors = adj_list.get(node_id, [])
                
                if neighbors:
                    # æ”¶é›†é‚»å±…åµŒå…¥ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰
                    embeddings_to_aggregate = []
                    
                    # æ·»åŠ è‡ªèº«åµŒå…¥
                    embeddings_to_aggregate.append(self.node_embeddings[node_id])
                    
                    # æ·»åŠ é‚»å±…åµŒå…¥
                    for neighbor_id in neighbors:
                        if neighbor_id in self.node_embeddings:
                            embeddings_to_aggregate.append(self.node_embeddings[neighbor_id])
                    
                    # èšåˆï¼šè®¡ç®—å¹³å‡å€¼ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
                    if embeddings_to_aggregate:
                        aggregated = np.mean(np.vstack(embeddings_to_aggregate), axis=0)
                        updated_embeddings[node_id] = aggregated
                else:
                    # æ²¡æœ‰é‚»å±…ï¼Œä¿æŒå½“å‰åµŒå…¥
                    updated_embeddings[node_id] = self.node_embeddings[node_id]
            
            # å½’ä¸€åŒ–å¹¶æ›´æ–°åµŒå…¥
            for node_id, embedding in updated_embeddings.items():
                norm = np.linalg.norm(embedding)
                if norm > 0:
                    self.node_embeddings[node_id] = embedding / norm
                else:
                    # é¿å…é›¶å‘é‡
                    self.node_embeddings[node_id] = embedding
        
        # Step 4: æ›´æ–°å—å½±å“èŠ‚ç‚¹çš„LSHç­¾å
        updated_count = 0
        for node_id in affected_nodes:
            if node_id in self.node_embeddings:
                self.aux.update_node_signature(node_id, self.node_embeddings[node_id])
                updated_count += 1
        
        logger.info(f"âœ… Updated Cleora embeddings for {updated_count} nodes using {self.cleora_iterations} iterations")

    async def _update_hierarchy_incremental(self):
        """
        å¢é‡æ›´æ–°å±‚æ¬¡åŒ–èšç±»ç»“æ„
        """
        logger.info("ğŸ”„ Updating hierarchy incrementally")
        
        # è®¡ç®—å—å½±å“èŠ‚ç‚¹çš„æ¯”ä¾‹
        total_nodes = len(self.aux.node_aux)
        affected_count = len(self.aux.affected_entities)
        affected_ratio = affected_count / total_nodes if total_nodes > 0 else 1.0
        
        logger.info(f"Affected ratio: {affected_ratio:.2%} ({affected_count}/{total_nodes})")
        
        # å¦‚æœå—å½±å“çš„èŠ‚ç‚¹å¤ªå¤šï¼Œæ‰§è¡Œå…¨é‡é‡æ„
        if affected_ratio > self.max_affected_ratio:
            logger.info(f"Affected ratio {affected_ratio:.2%} > threshold {self.max_affected_ratio:.2%}, performing full hierarchy rebuild")
            await self._rebuild_full_hierarchy()
            return
        
        # æ‰§è¡Œå¢é‡å±‚æ¬¡åŒ–æ›´æ–°
        await self._incremental_hierarchy_update()

    async def _incremental_hierarchy_update(self):
        """
        æ‰§è¡Œå¢é‡å±‚æ¬¡åŒ–æ›´æ–°
        """
        logger.info("ğŸ”§ Performing incremental hierarchy update")
        
        # ä»åº•å±‚å¼€å§‹ï¼Œé€å±‚å¤„ç†å—å½±å“çš„èŠ‚ç‚¹
        max_level = max(self.aux.level_to_nodes.keys()) if self.aux.level_to_nodes else 0
        
        for level in range(max_level + 1):#TODO
            affected_nodes = self.aux.get_affected_nodes_at_level(level)
            if not affected_nodes:
                continue
            
            logger.info(f"Processing level {level} with {len(affected_nodes)} affected nodes")
            
            if level == 0:
                # åº•å±‚ï¼šå¤„ç†æ–°åŠ å…¥çš„åŸºç¡€èŠ‚ç‚¹
                await self._process_level_0_incremental(affected_nodes)
            else:
                # ä¸Šå±‚ï¼šé‡æ–°èšç±»å—å½±å“çš„ç¤¾åŒº
                await self._process_upper_level_incremental(level, affected_nodes)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„é¡¶å±‚
        await self._check_and_create_new_top_level()

    async def _process_level_0_incremental(self, affected_nodes: List[str]):
        """
        å¤„ç†ç¬¬0å±‚ï¼ˆåŸºç¡€å±‚ï¼‰çš„å¢é‡æ›´æ–°
        """
        logger.info(f"Processing {len(affected_nodes)} affected base nodes")
        
        # è·å–è¿™äº›èŠ‚ç‚¹çš„åµŒå…¥
        affected_embeddings = []
        affected_node_ids = []
        
        for node_id in affected_nodes:
            if node_id in self.node_embeddings:
                affected_embeddings.append(self.node_embeddings[node_id])
                affected_node_ids.append(node_id)
        
        if not affected_embeddings:
            logger.warning("No embeddings found for affected nodes")
            return
        
        affected_embeddings = np.array(affected_embeddings)
        
        # å°è¯•å°†æ–°èŠ‚ç‚¹åˆ†é…åˆ°ç°æœ‰çš„1çº§ç¤¾åŒº
        level_1_communities = self.aux.get_valid_nodes_at_level(1)
        
        assignments = {}  # node_id -> community_id
        unassigned_nodes = []
        communities_to_update = set()  # éœ€è¦é‡æ–°ç”Ÿæˆæ‘˜è¦çš„ç¤¾åŒº
        
        for i, node_id in enumerate(affected_node_ids):
            node_embedding = affected_embeddings[i]
            node_signature = self.aux.signature_map.get(node_id)
            
            best_community = None
            best_similarity = -1
            
            # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ä¸”æœªæ»¡çš„ç¤¾åŒº
            for community_id in level_1_communities:
                if community_id in self.node_embeddings:
                    # æ£€æŸ¥ç¤¾åŒºå¤§å°é™åˆ¶
                    current_size = len(self.community_children.get(community_id, set()))
                    if current_size >= self.lsh_max_cluster_size:
                        #logger.debug(f"Community {community_id} is full ({current_size}/{self.lsh_max_cluster_size}), skipping")
                        continue
                    
                    community_embedding = self.node_embeddings[community_id]
                    similarity = np.dot(node_embedding, community_embedding)
                    
                    # æ£€æŸ¥LSHç­¾åå…¼å®¹æ€§
                    community_signature = self.aux.signature_map.get(community_id)
                    if node_signature and community_signature:
                        # è®¡ç®—æ±‰æ˜è·ç¦»
                        hamming_distance = bin(node_signature ^ community_signature).count('1')
                        # å¦‚æœæ±‰æ˜è·ç¦»è¿‡å¤§ï¼Œé™ä½ç›¸ä¼¼æ€§
                        if hamming_distance > self.lsh_num_hyperplanes // 4:
                            similarity *= 0.5 #TODO
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_community = community_id
            
            # å¦‚æœæ‰¾åˆ°åˆé€‚çš„ç¤¾åŒºä¸”ç›¸ä¼¼æ€§è¶³å¤Ÿé«˜
            if best_community and best_similarity > 0.5:
                assignments[node_id] = best_community
                #logger.debug(f"Assigned {node_id} to {best_community} (similarity: {best_similarity:.3f})")
                
                # æ›´æ–°ç¤¾åŒºçš„å­èŠ‚ç‚¹ä¿¡æ¯
                self.aux.set_parent_child_relationship(best_community, node_id)
                self.community_children[best_community].add(node_id)
                self.community_parents[node_id] = best_community
                
                # æ ‡è®°ç¤¾åŒºéœ€è¦æ›´æ–°
                self.aux.affected_entities.add(best_community)
                communities_to_update.add(best_community)
                
                # å…³é”®ä¿®å¤ï¼šå½“ç¤¾åŒºæ¥æ”¶æ–°æˆå‘˜æ—¶ï¼Œä¹Ÿéœ€è¦å‘ä¸Šä¼ æ’­å½±å“
                await self._propagate_impact_to_parent(best_community)
            else:
                unassigned_nodes.append(node_id)
        
        logger.info(f"Assigned {len(assignments)} nodes to existing communities, {len(unassigned_nodes)} nodes need new communities")
        
        # ä¸ºæ¥æ”¶äº†æ–°æˆå‘˜çš„ç¤¾åŒºé‡æ–°ç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        if communities_to_update:
            logger.info(f"Updating summaries for {len(communities_to_update)} communities that received new members")
            await self._update_communities_with_concurrency_control(list(communities_to_update), 0)
        
        # ä¸ºæœªåˆ†é…çš„èŠ‚ç‚¹åˆ›å»ºæ–°ç¤¾åŒºï¼ˆå¦‚æœæ•°é‡è¶³å¤Ÿï¼‰
        if len(unassigned_nodes) >= self.lsh_min_cluster_size:
            await self._create_new_communities_for_unassigned(unassigned_nodes, 0)

    async def _create_new_communities_for_unassigned(self, unassigned_nodes: List[str], level: int):
        """
        ä¸ºæœªåˆ†é…çš„èŠ‚ç‚¹åˆ›å»ºæ–°ç¤¾åŒºï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        """
        logger.info(f"Creating new communities for {len(unassigned_nodes)} unassigned nodes at level {level}")
        
        # è·å–æœªåˆ†é…èŠ‚ç‚¹çš„åµŒå…¥
        embeddings = []
        valid_node_ids = []
        for node_id in unassigned_nodes:
            if node_id in self.node_embeddings:
                embeddings.append(self.node_embeddings[node_id])
                valid_node_ids.append(node_id)
        
        if not embeddings:
            logger.warning("No embeddings found for unassigned nodes")
            return
        
        embeddings = np.array(embeddings)
        
        # å¯¹æœªåˆ†é…èŠ‚ç‚¹è¿›è¡ŒLSHèšç±»
        clusters = await self._lsh_clustering(embeddings, valid_node_ids)
        
        # ç­›é€‰å‡ºæ»¡è¶³æœ€å°å¤§å°è¦æ±‚çš„èšç±»
        valid_clusters = []
        current_level_communities = len(self.hierarchy_levels.get(level, []))
        for i, cluster_nodes in enumerate(clusters):
            if len(cluster_nodes) >= self.lsh_min_cluster_size:
                community_id = f"COMMUNITY_L{level}_C{current_level_communities + i}"
                valid_clusters.append((community_id, cluster_nodes, level))
            else:
                logger.debug(f"Cluster {i} too small ({len(cluster_nodes)} < {self.lsh_min_cluster_size}), skipping")
        
        if not valid_clusters:
            logger.info("No valid clusters created (all clusters too small)")
            return
        
        logger.info(f"Creating {len(valid_clusters)} new communities with concurrent processing")
        
        # ä½¿ç”¨å¹¶å‘æ§åˆ¶ç”Ÿæˆç¤¾åŒºæ‘˜è¦å’ŒåµŒå…¥
        await self._create_communities_with_concurrency_control(valid_clusters, level)
        
        logger.info(f"âœ… Successfully created {len(valid_clusters)} new communities at level {level}")

    async def _create_communities_with_concurrency_control(self, valid_clusters: List[Tuple[str, List[str], int]], level: int):
        """
        ä½¿ç”¨å¹¶å‘æ§åˆ¶åˆ›å»ºç¤¾åŒº
        
        Args:
            valid_clusters: [(community_id, cluster_nodes, level), ...]
            level: å½“å‰å±‚çº§
        """
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _create_single_community(community_data):
            community_id, cluster_nodes, community_level = community_data
            async with semaphore:
                try:
                    # ç”Ÿæˆç¤¾åŒºæ‘˜è¦å’ŒåµŒå…¥
                    await self._generate_community_summary_and_embedding(community_id, cluster_nodes, community_level)
                    
                    # æ›´æ–°å±‚æ¬¡ç»“æ„ - ä¿æŒä¸åŸå§‹æ ¼å¼ä¸€è‡´çš„å­—å…¸ç»“æ„
                    if level not in self.hierarchy_levels:
                        self.hierarchy_levels[level] = []
                    
                    community_data = {
                        'id': community_id,
                        'nodes': cluster_nodes,
                        'level': community_level
                    }
                    self.hierarchy_levels[level].append(community_data)
                    
                    # è®¾ç½®çˆ¶å­å…³ç³»
                    self.community_children[community_id] = set(cluster_nodes)
                    for child_id in cluster_nodes:
                        self.aux.set_parent_child_relationship(community_id, child_id)
                        self.community_parents[child_id] = community_id
                    
                    # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
                    self.aux.add_node_aux(community_id, 'community', level=level+1)
                    
                    # å…³é”®ä¿®å¤ï¼šæ–°åˆ›å»ºçš„ç¤¾åŒºä¹Ÿéœ€è¦å‘ä¸Šä¼ æ’­å½±å“
                    await self._propagate_impact_to_parent(community_id)
                    
                    logger.info(f"Created new community {community_id} with {len(cluster_nodes)} members")
                    return community_id
                    
                except Exception as e:
                    logger.error(f"Failed to create community {community_id}: {e}")
                    return None
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [_create_single_community(community_data) for community_data in valid_clusters]
        logger.info(f"ğŸ”§ Creating {len(tasks)} communities with max concurrency {max_concurrent}")
        
        # æ‰§è¡Œå¹¶å‘åˆ›å»º
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        successful_creations = 0
        failed_creations = 0
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Community creation failed with exception: {result}")
                failed_creations += 1
            elif result is not None:
                successful_creations += 1
            else:
                failed_creations += 1
        
        logger.info(f"ğŸ¯ Community creation completed: {successful_creations} successful, {failed_creations} failed")

    async def _process_upper_level_incremental(self, level: int, affected_nodes: List[str]):
        """
        å¤„ç†ä¸Šå±‚çš„å¢é‡æ›´æ–°
        """
        logger.info(f"Processing upper level {level} with {len(affected_nodes)} affected nodes")
        
        # Step 1: é‡æ–°ç”Ÿæˆå—å½±å“ç¤¾åŒºçš„æ‘˜è¦å’ŒåµŒå…¥ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
        await self._update_communities_with_concurrency_control(affected_nodes, level)
        
        # Step 2: å¤„ç†è¯¥å±‚æ–°å¢çš„ç¤¾åŒºèŠ‚ç‚¹ï¼Œéœ€è¦å°†å®ƒä»¬èšç±»åˆ°æ›´é«˜å±‚
        await self._process_new_communities_for_upper_clustering(level)

    async def _update_communities_with_concurrency_control(self, community_ids: List[str], level: int):
        """
        ä½¿ç”¨å¹¶å‘æ§åˆ¶æ›´æ–°ç¤¾åŒºæ‘˜è¦å’ŒåµŒå…¥
        """
        if not community_ids:
            return
            
        # ä½¿ç”¨ä¸çˆ¶ç±»ç›¸åŒçš„å¹¶å‘æ§åˆ¶å‚æ•°
        max_concurrent = getattr(self, 'max_concurrent_summaries', 35)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _update_single_community(community_id):
            async with semaphore:
                if community_id in self.community_children:
                    children = self.community_children[community_id]
                    await self._generate_community_summary_and_embedding(community_id, children, level)
                    # æ›´æ–°LSHç­¾å
                    if community_id in self.node_embeddings:
                        self.aux.update_node_signature(community_id, self.node_embeddings[community_id])
                    
                    # å…³é”®ä¿®å¤ï¼šæ ‡è®°è¯¥ç¤¾åŒºçš„çˆ¶èŠ‚ç‚¹ä¸ºå—å½±å“ï¼Œç¡®ä¿å½±å“å‘ä¸Šä¼ æ’­
                    await self._propagate_impact_to_parent(community_id)
        
        tasks = [_update_single_community(community_id) for community_id in community_ids]
        logger.info(f"ğŸ”§ Updating {len(tasks)} communities with max concurrency {max_concurrent}")
        await asyncio.gather(*tasks)
        
        # ç»Ÿè®¡å‘ä¸Šä¼ æ’­çš„å½±å“
        propagated_parents = set()
        for community_id in community_ids:
            parent_id = await self._get_community_parent_id(community_id)
            if parent_id:
                propagated_parents.add(parent_id)
        
        if propagated_parents:
            logger.info(f"ğŸ“ˆ Impact propagated to {len(propagated_parents)} parent communities at level {level + 1}")
        else:
            logger.debug(f"ğŸ” No parent communities to propagate impact to (reached top level)")

    async def _process_new_communities_for_upper_clustering(self, level: int):
        """
        å¤„ç†è¯¥å±‚æ–°å¢çš„ç¤¾åŒºèŠ‚ç‚¹ï¼Œå°†å®ƒä»¬èšç±»åˆ°æ›´é«˜å±‚ç¤¾åŒºä¸­
        
        Args:
            level: å½“å‰å¤„ç†çš„å±‚çº§
        """
        logger.info(f"Processing new communities at level {level} for upper clustering")
        
        # è·å–è¯¥å±‚æ‰€æœ‰ç¤¾åŒºèŠ‚ç‚¹ï¼ˆåŒ…æ‹¬æ–°å¢çš„ï¼‰
        current_level_communities = self.aux.get_valid_nodes_at_level(level)
        
        if not current_level_communities:
            logger.info(f"No communities found at level {level}")
            return
        
        # è¯†åˆ«æ–°å¢çš„ç¤¾åŒºï¼ˆæ²¡æœ‰çˆ¶èŠ‚ç‚¹çš„ç¤¾åŒºï¼‰
        new_communities = []
        for community_id in current_level_communities:
            parent_id = await self._get_community_parent_id(community_id)
            if not parent_id:  # æ²¡æœ‰çˆ¶èŠ‚ç‚¹è¯´æ˜æ˜¯æ–°å¢çš„ç¤¾åŒº
                new_communities.append(community_id)
        
        if not new_communities:
            logger.info(f"No new communities found at level {level} that need upper clustering")
            return
        
        logger.info(f"Found {len(new_communities)} new communities at level {level} that need upper clustering")
        
        # è·å–æ›´é«˜å±‚çš„ç°æœ‰ç¤¾åŒº
        upper_level = level + 1
        upper_level_communities = self.aux.get_valid_nodes_at_level(upper_level)
        
        # å°è¯•å°†æ–°ç¤¾åŒºåˆ†é…åˆ°ç°æœ‰çš„æ›´é«˜å±‚ç¤¾åŒº
        assignments = {}  # community_id -> parent_community_id
        unassigned_communities = []
        
        for community_id in new_communities:
            if community_id not in self.node_embeddings:
                logger.warning(f"Community {community_id} has no embedding, skipping")
                continue
                
            community_embedding = self.node_embeddings[community_id]
            community_signature = self.aux.signature_map.get(community_id)
            
            best_parent = None
            best_similarity = -1
            
            # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ä¸”æœªæ»¡çš„ä¸Šå±‚ç¤¾åŒº
            for parent_community_id in upper_level_communities:
                if parent_community_id in self.node_embeddings:
                    # æ£€æŸ¥çˆ¶ç¤¾åŒºå¤§å°é™åˆ¶
                    current_size = len(self.community_children.get(parent_community_id, set()))
                    if current_size >= self.lsh_max_cluster_size:
                        #logger.debug(f"Parent community {parent_community_id} is full ({current_size}/{self.lsh_max_cluster_size}), skipping")
                        continue
                    
                    parent_embedding = self.node_embeddings[parent_community_id]
                    similarity = np.dot(community_embedding, parent_embedding)
                    
                    # æ£€æŸ¥LSHç­¾åå…¼å®¹æ€§
                    parent_signature = self.aux.signature_map.get(parent_community_id)
                    if community_signature and parent_signature:
                        hamming_distance = bin(community_signature ^ parent_signature).count('1')
                        if hamming_distance > self.lsh_num_hyperplanes // 4:
                            similarity *= 0.5
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_parent = parent_community_id
            
            # å¦‚æœæ‰¾åˆ°åˆé€‚çš„çˆ¶ç¤¾åŒºä¸”ç›¸ä¼¼æ€§è¶³å¤Ÿé«˜
            if best_parent and best_similarity > 0.5:  # ä½¿ç”¨ç›¸åŒçš„é˜ˆå€¼
                assignments[community_id] = best_parent
                # è®¾ç½®çˆ¶å­å…³ç³»
                self.aux.set_parent_child_relationship(best_parent, community_id)
                self.community_parents[community_id] = best_parent
                if best_parent in self.community_children:
                    self.community_children[best_parent].add(community_id)
                else:
                    self.community_children[best_parent] = {community_id}
                
                # æ ‡è®°çˆ¶ç¤¾åŒºéœ€è¦æ›´æ–°
                self.aux.affected_entities.add(best_parent)
                # å‘ä¸Šä¼ æ’­å½±å“
                await self._propagate_impact_to_parent(best_parent)
                
                logger.info(f"Assigned community {community_id} to parent {best_parent} (similarity: {best_similarity:.3f})")
            else:
                unassigned_communities.append(community_id)
        
        logger.info(f"Assigned {len(assignments)} communities to existing upper communities, {len(unassigned_communities)} communities need new upper communities")
        
        # ä¸ºæœªåˆ†é…çš„ç¤¾åŒºåˆ›å»ºæ–°çš„æ›´é«˜å±‚ç¤¾åŒºï¼ˆå¦‚æœæ•°é‡è¶³å¤Ÿï¼‰
        if len(unassigned_communities) >= self.lsh_min_cluster_size:
            await self._create_new_communities_for_unassigned(unassigned_communities, upper_level-1)
        elif unassigned_communities:
            # å¦‚æœæœªåˆ†é…çš„ç¤¾åŒºæ•°é‡ä¸è¶³ä»¥åˆ›å»ºæ–°ç¤¾åŒºï¼Œä½†åˆä¸ä¸ºç©º
            # å¯ä»¥è€ƒè™‘é™ä½é˜ˆå€¼é‡æ–°åˆ†é…ï¼Œæˆ–è€…ç­‰å¾…æ›´å¤šç¤¾åŒº
            logger.info(f"Only {len(unassigned_communities)} unassigned communities, less than minimum cluster size {self.lsh_min_cluster_size}")

    async def _propagate_impact_to_parent(self, community_id: str):
        """
        å°†å½±å“ä¼ æ’­åˆ°çˆ¶èŠ‚ç‚¹
        
        Args:
            community_id: å½“å‰æ›´æ–°çš„ç¤¾åŒºID
        """
        try:
            # è·å–çˆ¶èŠ‚ç‚¹ID
            parent_id = await self._get_community_parent_id(community_id)
            
            if parent_id:
                # æ ‡è®°çˆ¶èŠ‚ç‚¹ä¸ºå—å½±å“
                self.aux.mark_node_affected(parent_id)
                
                logger.debug(f"Propagated impact from {community_id} to parent {parent_id}")
            else:
                logger.debug(f"Community {community_id} has no parent (top level)")
                
        except Exception as e:
            logger.warning(f"Failed to propagate impact from {community_id}: {e}")

    async def _get_community_parent_id(self, community_id: str) -> Optional[str]:
        """
        è·å–ç¤¾åŒºçš„çˆ¶èŠ‚ç‚¹ID
        
        Args:
            community_id: ç¤¾åŒºID
            
        Returns:
            Optional[str]: çˆ¶èŠ‚ç‚¹IDï¼Œå¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹åˆ™è¿”å›None
        """
        try:
            # æ–¹æ³•1ï¼šä»è¾…åŠ©ç»“æ„ä¸­è·å–
            if community_id in self.aux.node_aux:
                parent_id = self.aux.node_aux[community_id].parent
                if parent_id:
                    return parent_id
            
            # æ–¹æ³•2ï¼šä»community_parentsæ˜ å°„ä¸­è·å–
            parent_id = self.community_parents.get(community_id)
            if parent_id:
                return parent_id
            
            # æ–¹æ³•3ï¼šé€šè¿‡å±‚æ¬¡ç»“æ„æ¨æ–­ï¼ˆå¦‚æœå‰ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼‰
            # è§£æå½“å‰ç¤¾åŒºçš„å±‚çº§
            level_match = re.search(r'COMMUNITY_L(\d+)_C\d+', community_id)
            if level_match:
                current_level = int(level_match.group(1))
                parent_level = current_level + 1
                
                # æŸ¥æ‰¾å¯èƒ½çš„çˆ¶èŠ‚ç‚¹
                parent_communities = self.aux.get_valid_nodes_at_level(parent_level)
                for parent_candidate in parent_communities:
                    if parent_candidate in self.community_children:
                        if community_id in self.community_children[parent_candidate]:
                            return parent_candidate
            
            return None
            
        except Exception as e:
            logger.warning(f"Failed to get parent for community {community_id}: {e}")
            return None

    async def _check_and_create_new_top_level(self):
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ºæ–°å¢çš„ç¤¾åŒºåˆ›å»ºæ›´é«˜å±‚çš„é¡¶å±‚
        """
        logger.debug("Checking if new top level is needed")
        
        # è·å–å½“å‰æœ€é«˜å±‚çº§
        if not self.hierarchy_levels:
            return
        
        max_level = max(int(level) for level in self.hierarchy_levels.keys())
        #top_level_communities = self.hierarchy_levels.get(str(max_level), [])
        top_level_communities = self.hierarchy_levels.get(max_level)
        logger.info(f"Top level has {len(top_level_communities)} communities")
        
        # å¦‚æœé¡¶å±‚ç¤¾åŒºæ•°é‡è¿‡å¤šï¼Œè€ƒè™‘åˆ›å»ºæ–°çš„é¡¶å±‚
        if len(top_level_communities) > self.lsh_max_cluster_size:
            logger.info(f"Top level has {len(top_level_communities)} communities, considering creating new top level")
            
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§å±‚æ¬¡é™åˆ¶
            if max_level >= self.max_hierarchy_levels - 1:
                logger.info(f"Already at maximum hierarchy levels ({self.max_hierarchy_levels}), not creating new top level")
                return
            
            # è·å–é¡¶å±‚ç¤¾åŒºçš„åµŒå…¥
            top_communities_with_embeddings = []
            for community_data in top_level_communities:
                if isinstance(community_data, dict):
                    community_id = community_data.get('id')
                else:
                    community_id = community_data
                
                if community_id and community_id in self.node_embeddings:
                    top_communities_with_embeddings.append(community_id)
            
            if len(top_communities_with_embeddings) >= self.lsh_min_cluster_size:
                logger.info(f"Creating new top level with {len(top_communities_with_embeddings)} communities")
                
                # å¯¹é¡¶å±‚ç¤¾åŒºè¿›è¡Œèšç±»
                embeddings = np.array([self.node_embeddings[cid] for cid in top_communities_with_embeddings])
                clusters = await self._lsh_clustering(embeddings, top_communities_with_embeddings)
                
                new_level = max_level + 1
                new_communities = []
                
                # ä¸ºæ¯ä¸ªèšç±»åˆ›å»ºæ–°çš„é¡¶å±‚ç¤¾åŒº
                for cluster_id, cluster_nodes in enumerate(clusters):
                    if len(cluster_nodes) >= self.lsh_min_cluster_size:
                        new_community_id = f"COMMUNITY_L{new_level}_C{cluster_id}"
                        
                        # ç”Ÿæˆç¤¾åŒºæ‘˜è¦å’ŒåµŒå…¥
                        await self._generate_community_summary_and_embedding(new_community_id, cluster_nodes, new_level)
                        
                        # åˆ›å»ºç¤¾åŒºæ•°æ®
                        community_data = {
                            'id': new_community_id,
                            'nodes': cluster_nodes,
                            'level': new_level
                        }
                        new_communities.append(community_data)
                        
                        # è®¾ç½®çˆ¶å­å…³ç³»
                        self.community_children[new_community_id] = set(cluster_nodes)
                        for child_id in cluster_nodes:
                            self.community_parents[child_id] = new_community_id
                        
                        # æ·»åŠ åˆ°è¾…åŠ©ç»“æ„
                        self.aux.add_node_aux(new_community_id, 'community', level=new_level)
                        
                        logger.info(f"Created new top-level community {new_community_id} with {len(cluster_nodes)} children")
                
                # æ›´æ–°å±‚æ¬¡ç»“æ„
                if new_communities:
                    self.hierarchy_levels[new_level] = new_communities
                    logger.info(f"Created new hierarchy level {new_level} with {len(new_communities)} communities")
        else:
            logger.info(f"Top level has {len(top_level_communities)} communities, no new top level needed")

    async def _rebuild_full_hierarchy(self):
        """
        æ‰§è¡Œå®Œæ•´çš„å±‚æ¬¡ç»“æ„é‡æ„
        """
        logger.info("ğŸ”„ Rebuilding full hierarchy due to large number of affected nodes")
        
        # æ¸…é™¤ç°æœ‰å±‚æ¬¡ç»“æ„
        self.hierarchy_levels.clear()
        self.community_summaries.clear()
        self.community_children.clear()
        self.community_parents.clear()
        
        # é‡æ–°æ„å»ºå±‚æ¬¡ç»“æ„
        await self._build_hierarchy()

    async def _update_faiss_indexes_incremental(self):
        """
        å¢é‡æ›´æ–°FAISSç´¢å¼•
        """
        logger.info("ğŸ”„ Updating FAISS indexes incrementally")
        
        # è·å–æ‰€æœ‰å—å½±å“çš„èŠ‚ç‚¹
        affected_nodes = list(self.aux.affected_entities)
        if not affected_nodes:
            logger.info("No affected nodes, skipping FAISS index update")
            return
        
        # é‡æ–°æ„å»ºFAISSç´¢å¼•ï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥åªæ›´æ–°å—å½±å“çš„éƒ¨åˆ†
        await self._build_faiss_index()
        
        logger.info("âœ… FAISS indexes updated")

    async def _save_incremental_updates(self):
        """
        ä¿å­˜å¢é‡æ›´æ–°çš„ç»“æœ
        """
        logger.info("ğŸ’¾ Saving incremental updates")
        
        # ä¿å­˜å›¾ç»“æ„
        await self._graph.persist(force=True)
        
        # ä¿å­˜å±‚æ¬¡ç»“æ„æ•°æ®
        await self._save_hierarchy_to_storage(force=True)
        
        # ä¿å­˜è¾…åŠ©æ•°æ®
        self.aux.save_aux_data()
        
        # ä¿å­˜FAISSç´¢å¼•
        await self._save_faiss_indexes()
        
        logger.info("âœ… Incremental updates saved successfully")

    def get_incremental_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¢é‡æ›´æ–°çš„ç»Ÿè®¡ä¿¡æ¯
        """
        stats = self.aux.get_statistics()
        stats.update({
            'incremental_mode': self.incremental_mode,
            'base_hierarchy_built': self.base_hierarchy_built,
            'enable_incremental_update': self.enable_incremental_update,
            'hierarchy_levels': len(self.hierarchy_levels),
            'total_communities': sum(len(communities) for communities in self.hierarchy_levels.values()),
            'total_node_embeddings': len(self.node_embeddings),
            'total_signatures': len(self.aux.signature_map)
        })
        return stats
