import asyncio
from typing import List, Dict, Any, Optional

from Core.Query.BaseQuery import BaseQuery
from Core.Common.Logger import logger
from Core.Prompt import QueryPrompt, GraphPrompt
from Core.Retriever.HKGraphTreeRetriever import HKGraphTreeRetriever
from Core.Query.TripleExtractor import TripleExtractor
from Core.Common.Utils import truncate_list_by_token_size, list_to_quoted_csv_string, prase_json_from_response, clean_str
from Core.Common.Constants import Retriever


class HKGraphTreeQuery(BaseQuery):
    """
    HKGraphTreeä¸“ç”¨æŸ¥è¯¢å™¨
    
    æ”¯æŒåŸºäºŽå±‚æ¬¡åŒ–ç¤¾åŒºç»“æž„çš„ä»Žé¡¶å‘ä¸‹æ£€ç´¢å’ŒæŸ¥è¯¢å¤„ç†
    """
    
    def __init__(self, config, retriever_context):
        super().__init__(config, retriever_context)
        
        # ç›´æŽ¥ä»Žretriever_contextèŽ·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        # RetrieverContext.contextæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æ³¨å†Œçš„ä¸Šä¸‹æ–‡
        contexts = retriever_context.context
        
        # åˆå§‹åŒ–TripleExtractorï¼Œä¼ å…¥å¿…éœ€çš„llmå‚æ•°
        self.triple_extractor = TripleExtractor(
            llm=self.llm,
            entities_vdb=contexts.get('entities_vdb'),
            graph=contexts.get('graph'),
            doc_chunk=contexts.get('doc_chunk')
        )
        
        # åˆ›å»ºHKGraphTreeä¸“ç”¨æ£€ç´¢å™¨
        # ä»Žcontextsä¸­ç§»é™¤configï¼Œé¿å…é‡å¤ä¼ é€’
        retriever_contexts = {k: v for k, v in contexts.items() if k != 'config'}
        self.tree_retriever = HKGraphTreeRetriever(
            config=config,
            **retriever_contexts
        )

    async def _retrieve_relevant_contexts(self, query: str, **kwargs) -> str:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ - BaseQueryçš„æŠ½è±¡æ–¹æ³•å®žçŽ°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
            
        Returns:
            æž„å»ºçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        
        try:
            # Step 1: æå–æŸ¥è¯¢å®žä½“ï¼ˆå¦‚æžœéœ€è¦ï¼‰
            # query_entities = await self.extract_query_entities(query)
            query_entities = [] 

            # Step 2: æ‰§è¡Œå±‚æ¬¡åŒ–å›¾æ£€ç´¢
            retrieval_results = await self._execute_hierarchical_retrieval(
                query, query_entities
            )
            
            # Step 3: æž„å»ºä¸Šä¸‹æ–‡
            context = await self._build_context_from_results(retrieval_results)
            
            #logger.info("âœ… HKGraphTreeä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ")
            return context
            
        except Exception as e:
            logger.error(f"âŒ HKGraphTree context retrieval failed: {e}")
            return f"Context retrieval failed: {str(e)}"

    async def query(self, question: str) -> str:
        """
        HKGraphTreeçš„ä¸»æŸ¥è¯¢æ–¹æ³•ï¼ˆé‡å†™BaseQueryçš„queryæ–¹æ³•ï¼‰
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
            
        Returns:
            ç”Ÿæˆçš„å›žç­”
        """
        logger.info(f"ðŸŒ² HKGraphTreeæŸ¥è¯¢å¼€å§‹: {question[:100]}...")
        
        try:
            # èŽ·å–ä¸Šä¸‹æ–‡
            context = await self._retrieve_relevant_contexts(question)
            
            # æ ¹æ®æŸ¥è¯¢ç±»åž‹ç”Ÿæˆå›žç­”
            if self.config.query_type == "summary":
                response = await self.generation_summary(question, context)
            elif self.config.query_type == "qa":
                response = await self.generation_qa(question, context)
            else:
                logger.error("Invalid query type")
                response = "Unsupported query type"
            
            #logger.debug("âœ… HKGraphTreeæŸ¥è¯¢å®Œæˆ")
            return response
            
        except Exception as e:
            logger.error(f"âŒ HKGraphTree query failed: {e}")
            return f"Query processing failed: {str(e)}"

    async def _execute_hierarchical_retrieval(self, question: str, query_entities: List[Dict]) -> Dict[str, Any]:
        """
        Execute retrieval
        """
        
        logger.info(f"ðŸ”§ ä½¿ç”¨æ£€ç´¢æ–¹æ³•: hk_tree_flat_search")
        
        try:
            # æ£€æŸ¥tree_retrieveræ˜¯å¦å¯ç”¨
            if not hasattr(self, 'tree_retriever') or self.tree_retriever is None:
                logger.error("âŒ tree_retrieveræœªæ­£ç¡®åˆå§‹åŒ–")
                return self._get_empty_results()
                
            results = None
            
            try:
                if hasattr(self.tree_retriever, '_hk_tree_flat_search_retrieval'):
                    results = await self.tree_retriever._hk_tree_flat_search_retrieval(question, query_entities)
                else:
                    logger.warning("âš ï¸ flat_searchæ£€ç´¢æ–¹æ³•ä¸å­˜åœ¨ï¼Œä½¿ç”¨å›žé€€æ–¹æ³•")
                    results = await self._fallback_retrieval(question, query_entities)
                        
            except Exception as retrieval_error:
                logger.error(f"âŒ å±‚æ¬¡åŒ–æ£€ç´¢æ–¹æ³•æ‰§è¡Œå¤±è´¥: {retrieval_error}")
                logger.info("ðŸ”„ è½¬ä¸ºä½¿ç”¨å›žé€€æ£€ç´¢æ–¹æ³•...")
                results = await self._fallback_retrieval(question, query_entities)
            
            # ç¡®ä¿resultsä¸ä¸ºNone
            if results is None:
                logger.warning("âš ï¸ æ£€ç´¢æ–¹æ³•è¿”å›žNoneï¼Œä½¿ç”¨å›žé€€ç»“æžœ")
                results = await self._fallback_retrieval(question, query_entities)
            
            return results
            
        except Exception as e:
            logger.error(f"Hierarchical retrieval failed: {e}")
            return self._get_empty_results()

    def _validate_retrieval_results(self, results: Dict[str, Any]) -> bool:
        """
        éªŒè¯æ£€ç´¢ç»“æžœæ˜¯å¦æœ‰æ•ˆ
        
        Args:
            results: æ£€ç´¢ç»“æžœ
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not results or not isinstance(results, dict):
            return False
        
        # æ£€æŸ¥å¿…è¦çš„å­—æ®µæ˜¯å¦å­˜åœ¨
        required_fields = ['communities', 'entities', 'chunks', 'relationships', 'community_summaries']
        for field in required_fields:
            if field not in results:
                logger.warning(f"Missing required field: {field}")
                return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
        total_content = (len(results.get('communities', [])) + 
                        len(results.get('entities', [])) + 
                        len(results.get('chunks', [])))
        
        if total_content == 0:
            logger.warning("No content found in retrieval results")
            return False
        
        return True

    def _get_empty_results(self) -> Dict[str, Any]:
        """
        èŽ·å–ç©ºçš„æ£€ç´¢ç»“æžœ
        """
        return {
            'communities': [],
            'entities': [],
            'chunks': [],
            'relationships': [],
            'community_summaries': []
        }
    
    async def _fallback_retrieval(self, question: str, query_entities: List[Dict]) -> Dict[str, Any]:
        """
        å›žé€€æ£€ç´¢æ–¹æ³•ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç­–ç•¥
        """
        logger.info("ðŸ”„ ä½¿ç”¨å›žé€€æ£€ç´¢æ–¹æ³•...")
        
        try:
            results = self._get_empty_results()
            
            # 1. å¤„ç†æŸ¥è¯¢å®žä½“
            if query_entities:
                # è¿‡æ»¤æŽ‰å¤ªçŸ­çš„å®žä½“
                valid_entities = []
                for entity in query_entities:
                    if isinstance(entity, dict):
                        entity_name = entity.get('entity_name', '')
                        if len(entity_name) > 2 and entity_name not in ['the', 'and', 'for', 'are', 'was', 'were']:
                            valid_entities.append(entity)
                    elif isinstance(entity, str) and len(entity) > 2:
                        valid_entities.append({
                            'entity_name': entity,
                            'entity_type': 'EXTRACTED',
                            'description': f'ä»ŽæŸ¥è¯¢ä¸­æå–çš„å®žä½“: {entity}'
                        })
                
                results['entities'] = valid_entities[:5]
                logger.info(f"ðŸ“ å›žé€€æ£€ç´¢è¿”å›žäº† {len(results['entities'])} ä¸ªæœ‰æ•ˆå®žä½“")
            
            # 2. å°è¯•ä»Žå·²æž„å»ºçš„å›¾ä¸­èŽ·å–ä¸€äº›ç¤ºä¾‹æ•°æ®
            try:
                if hasattr(self, 'tree_retriever') and hasattr(self.tree_retriever, 'graph'):
                    graph = self.tree_retriever.graph
                    if graph:
                        # å°è¯•èŽ·å–å›¾çš„åŸºæœ¬ä¿¡æ¯
                        try:
                            all_nodes = await graph.get_nodes()
                            if all_nodes:
                                # èŽ·å–å‰å‡ ä¸ªå®žä½“èŠ‚ç‚¹ä½œä¸ºç¤ºä¾‹
                                sample_entity_nodes = []
                                for node_id in list(all_nodes)[:10]:
                                    if not node_id.startswith('CHUNK_') and not node_id.startswith('COMMUNITY_'):
                                        node_data = await graph.get_node(node_id)
                                        if node_data:
                                            sample_entity_nodes.append({
                                                'entity_name': node_data.get('entity_name', node_id),
                                                'entity_type': node_data.get('entity_type', 'GRAPH_ENTITY'),
                                                'description': node_data.get('description', 'å›¾ä¸­çš„å®žä½“'),
                                                'source': 'graph_sample'
                                            })
                                
                                if sample_entity_nodes:
                                    results['entities'].extend(sample_entity_nodes[:3])
                                    logger.info(f"ðŸ“Š ä»Žå›¾ä¸­æ·»åŠ äº† {len(sample_entity_nodes[:3])} ä¸ªç¤ºä¾‹å®žä½“")
                        except Exception as e:
                            logger.warning(f"èŽ·å–å›¾èŠ‚ç‚¹å¤±è´¥: {e}")
            except Exception as e:
                logger.warning(f"è®¿é—®å›¾å¤±è´¥: {e}")
            
            # 3. å°è¯•èŽ·å–æ–‡æ¡£å—
            try:
                if hasattr(self, 'tree_retriever') and hasattr(self.tree_retriever, 'doc_chunk'):
                    doc_chunk = self.tree_retriever.doc_chunk
                    if doc_chunk:
                        sample_chunks = []
                        
                        # å°è¯•ä¸åŒçš„æ–¹æ³•èŽ·å–æ–‡æ¡£å—
                        chunk_keys = []
                        if hasattr(doc_chunk, 'get_all_keys'):
                            try:
                                chunk_keys = await doc_chunk.get_all_keys()
                            except:
                                pass
                        
                        # å¦‚æžœæ²¡æœ‰keysï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
                        if not chunk_keys:
                            for i in range(5):  # å°è¯•å‰5ä¸ªç´¢å¼•
                                try:
                                    chunk_content = await doc_chunk.get_data_by_index(i)
                                    if chunk_content:
                                        sample_chunks.append({
                                            'id': str(i),
                                            'content': chunk_content[:800],  # å¢žåŠ å†…å®¹é•¿åº¦
                                            'type': 'chunk'
                                        })
                                except:
                                    continue
                        else:
                            # ä½¿ç”¨keysèŽ·å–å†…å®¹
                            for i, key in enumerate(chunk_keys[:3]):
                                try:
                                    chunk_content = await doc_chunk.get_data_by_key(key)
                                    if chunk_content:
                                        sample_chunks.append({
                                            'id': key,
                                            'content': chunk_content[:800],  # å¢žåŠ å†…å®¹é•¿åº¦
                                            'type': 'chunk'
                                        })
                                except:
                                    continue
                        
                        results['chunks'] = sample_chunks
                        logger.info(f"ðŸ“„ å›žé€€æ£€ç´¢è¿”å›žäº† {len(sample_chunks)} ä¸ªæ–‡æ¡£å—")
            except Exception as e:
                logger.warning(f"èŽ·å–æ–‡æ¡£å—å¤±è´¥: {e}")
            
            # 4. ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›åŸºç¡€ä¿¡æ¯
            if not results['entities'] and not results['chunks']:
                # å¦‚æžœä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œè‡³å°‘æä¾›æŸ¥è¯¢å…³é”®è¯ä½œä¸ºå®žä½“
                query_words = [word.strip() for word in question.split() 
                              if len(word.strip()) > 3 and word.strip().lower() not in 
                              ['what', 'where', 'when', 'who', 'why', 'how', 'does', 'was', 'were', 'are', 'the', 'and', 'for']]
                
                fallback_entities = []
                for word in query_words[:5]:
                    fallback_entities.append({
                        'entity_name': word,
                        'entity_type': 'KEYWORD',
                        'description': f'ä»ŽæŸ¥è¯¢ä¸­æå–çš„å…³é”®è¯: {word}'
                    })
                
                results['entities'] = fallback_entities
                logger.info(f"ðŸŽ¯ ä½¿ç”¨å…³é”®è¯å›žé€€ï¼Œæå–äº† {len(fallback_entities)} ä¸ªå…³é”®è¯å®žä½“")
            
            total_content = len(results['entities']) + len(results['chunks'])
            logger.info(f"âœ… å›žé€€æ£€ç´¢å®Œæˆï¼Œæ€»å…±è¿”å›ž {total_content} é¡¹å†…å®¹")
            
            return results
            
        except Exception as e:
            logger.error(f"å›žé€€æ£€ç´¢ä¹Ÿå¤±è´¥äº†: {e}")
            # æœ€åŽçš„æœ€åŽï¼Œè‡³å°‘è¿”å›žä¸€äº›æŸ¥è¯¢å…³é”®è¯
            try:
                query_words = [word.strip() for word in question.split() if len(word.strip()) > 3][:3]
                fallback_entities = [{'entity_name': word, 'entity_type': 'KEYWORD', 'description': f'å…³é”®è¯: {word}'} for word in query_words]
                return {
                    'communities': [],
                    'entities': fallback_entities,
                    'chunks': [],
                    'relationships': [],
                    'community_summaries': []
                }
            except:
                return self._get_empty_results()

    async def _build_context_from_results(self, retrieval_results: Dict[str, Any]) -> str:
        """
        ä»ŽRAPTORå¼æ£€ç´¢ç»“æžœæž„å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡
        """
        #logger.info("ðŸ“ æž„å»ºRAPTORå¼æŸ¥è¯¢ä¸Šä¸‹æ–‡...")
        
        context_parts = []
        max_context_length = getattr(self.config, 'max_token_for_text_unit', 4000) * 10 #TODO context æˆªæ–­
        
        # 1. å±‚æ¬¡åŒ–ç¤¾åŒºä¿¡æ¯ï¼ˆæŒ‰å±‚æ¬¡å’Œç›¸ä¼¼æ€§æŽ’åºï¼‰
        communities = retrieval_results.get('communities', [])
        if communities:
            context_parts.append("=== Hierarchical Community Analysis ===")
            
            # æŒ‰å±‚æ¬¡åˆ†ç»„
            level_groups = {}
            for community in communities:
                level = community.get('level', 0)
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(community)
            
            # ä»Žé«˜å±‚åˆ°ä½Žå±‚å±•ç¤º
            for level in sorted(level_groups.keys(), reverse=True):
                level_communities = level_groups[level]
                # æŒ‰ç›¸ä¼¼æ€§åˆ†æ•°æŽ’åº
                level_communities.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
                
                context_parts.append(f"Level {level} Communities:")
                for i, community in enumerate(level_communities[:1]):  # æ¯å±‚æœ€å¤š1ä¸ª
                    score = community.get('similarity_score', 0)
                    context_parts.append(f"  Community {i+1} (Score: {score:.3f}, Members: {community.get('member_count', 0)}):")
                    if community.get('summary'):
                        summary = community['summary']
                        if len(summary) > 3000:
                            summary = summary[:3000] + "..."
                        context_parts.append(f"    {summary}")
                context_parts.append("")
        
        # 2. é«˜ç›¸ä¼¼æ€§å®žä½“ä¿¡æ¯
        entities = retrieval_results.get('entities', [])
        if entities:
            context_parts.append("=== Most Relevant Entities ===")
            # å®žä½“å·²ç»æŒ‰ç›¸ä¼¼æ€§æŽ’åºï¼Œç›´æŽ¥ä½¿ç”¨
            for i, entity in enumerate(entities):
                entity_name = entity.get('entity_name', 'N/A')
                entity_type = entity.get('entity_type', '')
                score = entity.get('similarity_score', 0)
                description = entity.get('description', '')
                
                entity_info = f"{i+1}. {entity_name}"
                if entity_type:
                    entity_info += f" ({entity_type})"
                if score > 0:
                    entity_info += f" [Score: {score:.3f}]"
                context_parts.append(entity_info)
                
                if description:
                    # æˆªæ–­è¿‡é•¿çš„æè¿°
                    if len(description) > 150:
                        description = description[:150] + "..."
                    context_parts.append(f"   Description: {description}")
            context_parts.append("")
        
        # 3. å…³ç³»ç½‘ç»œä¿¡æ¯
        relationships = retrieval_results.get('relationships', [])
        if relationships:
            context_parts.append("=== Key Relationships ===")
            for i, rel in enumerate(relationships):
                src_id = rel.get('src_id', 'N/A')
                tgt_id = rel.get('tgt_id', 'N/A')
                relation_name = rel.get('relation_name', 'N/A')
                description = rel.get('description', '')
                
                rel_info = f"{i+1}. {src_id} --[{relation_name}]--> {tgt_id}"
                context_parts.append(rel_info)
                
                if description:
                    if len(description) > 100:
                        description = description[:100] + "..."
                    context_parts.append(f"   Context: {description}")
            context_parts.append("")
        
        # 4. æœ€ç›¸å…³çš„æ–‡æ¡£å†…å®¹
        chunks = retrieval_results.get('chunks', [])
        if chunks:
            context_parts.append("=== Most Relevant Documents ===")
            # æ–‡æ¡£å—å·²ç»æŒ‰ç›¸ä¼¼æ€§æŽ’åº
            for i, chunk in enumerate(chunks):#TODO
                score = chunk.get('similarity_score', 0)
                content = chunk.get('content', '')

                context = "\n".join(context_parts)  # contextå®Œæ•´å†…å®¹æˆªæ–­
                if len(content) + len(context) > max_context_length:
                    break
                
                context_parts.append(f"Document {i+1} [Score: {score:.3f}]:")
                
                # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™é‡è¦éƒ¨åˆ† - å¯é…ç½®ç‰ˆæœ¬
                max_full_length = getattr(self.config, 'max_document_display_length', 8000)  # å…è®¸å®Œæ•´æ˜¾ç¤ºçš„æœ€å¤§é•¿åº¦
                max_smart_truncate_length = getattr(self.config, 'max_smart_truncate_length', 8000)  # æ™ºèƒ½æˆªæ–­çš„é˜ˆå€¼
                head_chars = getattr(self.config, 'truncate_head_chars', 4000)  # ä¿ç•™å¼€å¤´å­—ç¬¦æ•°
                tail_chars = getattr(self.config, 'truncate_tail_chars', 3000)  # ä¿ç•™ç»“å°¾å­—ç¬¦æ•°
                
                if len(content) > max_full_length:
                    # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å¼€å¤´å’Œç»“å°¾
                    content = content[:head_chars] + "\n...[content truncated]...\n" + content[-tail_chars:]
                elif len(content) > max_smart_truncate_length:
                    # ç®€å•æˆªæ–­ï¼šåªä¿ç•™å¼€å¤´
                    content = content[:head_chars] + "..."

                context_parts.append(content)
                context_parts.append("")
        
        # 5. æ·»åŠ æ£€ç´¢å…ƒä¿¡æ¯
        total_items = len(communities) + len(entities) + len(chunks) + len(relationships)
        #total_items = 1 + len(entities) + 4 + len(relationships)
        if total_items > 0:
            context_parts.append("=== Retrieval Summary ===")
            context_parts.append(f"Retrieved {len(communities)} communities across {len(set(c.get('level', 0) for c in communities))} levels, "
                                f"{len(entities)} entities, {len(chunks)} documents, and {len(relationships)} relationships "
                                f"using  hierarchical retrieval.")
            # context_parts.append(f"Retrieved 1 communities across {len(set(c.get('level', 0) for c in communities))} levels, "
            #                     f"{len(entities)} entities, 4 documents, and {len(relationships)} relationships "
            #                     f"using hierarchical retrieval.")
            context_parts.append("")
        
        context = "\n".join(context_parts)
        
        # é™åˆ¶æ€»é•¿åº¦
        max_context_length = getattr(self.config, 'max_token_for_text_unit', 4000) * 10 #TODO context æˆªæ–­
        if len(context) > max_context_length:
            context = context[:max_context_length] + "\n...(content truncated for length)"
        
        logger.info(f"ðŸ“‹ HKGraphTreeLSHä¸Šä¸‹æ–‡æž„å»ºå®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦ï¼ŒåŒ…å« {total_items} é¡¹å†…å®¹")
        return context

    async def generation_qa(self, query: str, context: str) -> str:
        """
        ç”Ÿæˆé—®ç­”å›žå¤ - BaseQueryçš„æŠ½è±¡æ–¹æ³•å®žçŽ°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            ç”Ÿæˆçš„å›žç­”
        """
        logger.debug("ðŸ¤– å¼€å§‹ç”Ÿæˆé—®ç­”å›žå¤...")
        
        if not context or context.strip() == "":
            return "Sorry, no relevant information was found to answer your question."
        
        try:
            # æž„å»ºæç¤ºè¯
            system_prompt = self._build_system_prompt_for_qa_prompt_options_analyze_nm() #TODO æç¤ºè¯
            #system_prompt = ""
            # Build user message
            user_message = f"""
Based on the following context information, please answer the user's question.

Context information:
{context}

User question: {query}

Give the best full answer amongst the option to question.(if the question is a option chosing question)
According to the retrieved context, please provide detailed and accurate answers.If the context does not contain sufficient information to answer the question, please state "Insufficient information". When possible, reference specific information from the context.
"""
            
            # è°ƒç”¨LLMç”Ÿæˆå›žç­” # system_msgs=[system_prompt] if system_prompt else None 
            if hasattr(self, 'llm') and self.llm:
                #print(f"system_prompt: {system_prompt}\n")
                # print(f"user_message: {user_message}\n") #TODO debuging
                #print("user_message: {}\n".format(user_message.replace('\n', '\\n')))
                response = await self.llm.aask(
                    user_message, system_msgs=[system_prompt] if system_prompt else None
                    
                )
            else:
                # Simple template response
                response = f"Based on the retrieved information, here is the answer to the question '{query}':\n\n{context[:200]}..."
            
            logger.debug("âœ… é—®ç­”å›žå¤ç”Ÿæˆå®Œæˆ")
            #print(f"response: {response}\n") #TODO debuging
            return response
            
        except Exception as e:
            logger.error(f"Q&A response generation failed: {e}")
            return f"Sorry, an error occurred while generating the answer: {str(e)}"

    async def generation_summary(self, query: str, context: str) -> str:
        """
        ç”Ÿæˆæ‘˜è¦ - BaseQueryçš„æŠ½è±¡æ–¹æ³•å®žçŽ°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            ç”Ÿæˆçš„æ‘˜è¦
        """
        logger.info("ðŸ“‹ å¼€å§‹ç”Ÿæˆæ‘˜è¦...")
        
        if not context or context.strip() == "":
            return "Sorry, no relevant information was found to generate a summary."
        
        try:
            # æž„å»ºæ‘˜è¦æç¤ºè¯
            system_prompt = self._build_system_prompt_for_summary() #TODO
            
            # Build user message
            user_message = f"""
Based on the following context information, generate a concise summary to answer the query.

Context information:
{context}

Query topic: {query}

Please generate a concise and comprehensive summary that highlights the most relevant points to the query.
"""
            
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            if hasattr(self, 'llm') and self.llm:
                response = await self.llm.aask(
                    user_message,
                    system_msgs=[system_prompt] if system_prompt else None
                )
            else:
                # Simple template summary
                response = f"Summary based on hierarchical retrieval:\n\n{context[:300]}..."
            
            logger.info("âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            return response
            
        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            return f"Sorry, an error occurred while generating the summary: {str(e)}"



    def _build_system_prompt_for_qa_new_prompt_options(self) -> str:
        """
        Build system prompt for Q&A (after modified)
        """
        system_prompt = """
        You are an intelligent RAG Q&A assistant using hierarchical knowledge graphs.

Rules:
1. Consider **Entities**, **Key Relationships**, **Documents**, and **Community Summaries** together.  
2. If a fact appears in **Key Relationships**, treat it as reliable even if not repeated elsewhere.  
3. Use **Documents** for context or confirmation, but do not require them to validate relationship facts.  
4. Report consistency across sources; if sources conflict, describe the discrepancy.  
5. If none of the sections provide sufficient relevant evidence, explicitly state â€œInsufficient informationâ€. After that, you may provide a plausible guess or hypothesis, clearly labeling it as a guess and separating it from the evidence-based answer. 
6. Always state which section(s) support your answer.
        """
#7. Give the best full answer amongst the option to question.

        return system_prompt
    
    def _build_system_prompt_for_qa_prompt_options_analyze(self) -> str:  #TODO claude prompt analyze
        """
        Build system prompt for Q&A (after modified)
        """
        system_prompt = """
You are an intelligent RAG Q&A assistant using hierarchical knowledge graphs.

Rules:
1. Consider **Entities**, **Key Relationships**, **Documents**, and **Community Summaries** together.  
2. If a fact appears in **Key Relationships**, treat it as the most reliable source of truth, even if it seems unusual or is not repeated elsewhere. Do not override it with everyday common-sense assumptions.  
3. Use **Documents** for context or confirmation, but do not require them to validate relationship facts.  
4. Report consistency across sources; if sources conflict, describe the discrepancy.  
5. If none of the sections provide sufficient relevant evidence, explicitly state â€œInsufficient informationâ€. After that, you may provide a plausible guess or hypothesis, clearly labeling it as a guess and separating it from the evidence-based answer.  
6. You need to analyze based on the original text, not over-interpret it.

Response format: First analyze the evidence and reasoning process, then provide your answer with source attribution.

"""

        return system_prompt

    def _build_system_prompt_for_qa_prompt_options_analyze_nm(self) -> str:  #TODO claude prompt analyze
        """
        Build system prompt for Q&A (after modified)
        """
        system_prompt = """
You are an intelligent RAG Q&A assistant using hierarchical knowledge graphs.

Rules:
1. Consider **Entities**, **Key Relationships**, **Documents**, and **Community Summaries** together.  
2. If a fact appears in **Key Relationships**, treat it as the most reliable source of truth, even if it seems unusual or is not repeated elsewhere. Do not override it with everyday common-sense assumptions.  
3. Use **Documents** for context or confirmation, but do not require them to validate relationship facts.  
4. Report consistency across sources; if sources conflict, describe the discrepancy.  
5. Do not make up information.
6. You need to analyze based on the original text, not over-interpret it.

Response format: First analyze the evidence and reasoning process, then provide your answer with source attribution.

"""

        return system_prompt

    def _build_system_prompt_for_qa_prompt_options_analyze_noII(self) -> str:  #TODO claude prompt analyze
        """
        Build system prompt for Q&A (after modified)
        """
        system_prompt = """
You are an intelligent RAG Q&A assistant using hierarchical knowledge graphs.

Rules:
1. Consider **Entities**, **Key Relationships**, **Documents**, and **Community Summaries** together.  
2. If a fact appears in **Key Relationships**, treat it as the most reliable source of truth, even if it seems unusual or is not repeated elsewhere. Do not override it with everyday common-sense assumptions.  
3. Use **Documents** for context or confirmation, but do not require them to validate relationship facts.  
4. Report consistency across sources; if sources conflict, describe the discrepancy.  
5. Do not make up. 
6. You need to analyze based on the original text, not over-interpret it.

Response format: First analyze the evidence and reasoning process, then provide your answer with source attribution.

"""

        return system_prompt

    def _build_system_prompt_for_qa_prompt_options_analyze_debug(self) -> str:  #TODO claude prompt analyze
        """
        Build system prompt for Q&A (after modified)
        """
        system_prompt = """
You are an intelligent RAG Q&A assistant using hierarchical knowledge graphs.

Rules:
1. Consider **Entities**, **Key Relationships**, **Documents**, and **Community Summaries** together.  
2. If a fact appears in **Key Relationships**, treat it as the most reliable source of truth, even if it seems unusual or is not repeated elsewhere. Do not override it with everyday common-sense assumptions.  
3. Use **Documents** for context or confirmation, but do not require them to validate relationship facts.  
4. Report consistency across sources; if sources conflict, describe the discrepancy.  
5. If none of the sections provide sufficient relevant evidence, explicitly state â€œInsufficient informationâ€. After that, you may provide a plausible guess or hypothesis, clearly labeling it as a guess and separating it from the evidence-based answer.  
6. You need to analyze based on the original text, not over-interpret it.

Response format: First analyze the evidence and reasoning process, then provide your answer with source attribution.

"""

        return system_prompt

    def _build_system_prompt_for_summary(self) -> str:
        """
        Build system prompt for summary
        """
        system_prompt = """You are a professional information summarization expert specializing in organizing information based on hierarchical knowledge graphs.

Summary requirements:
1. Extract the most core topics and concepts
2. Maintain hierarchical structure and logical relationships
3. Highlight key entities and their important relationships
4. Be concise and clear, avoiding redundant information
5. Maintain an objective and neutral tone

Please generate well-structured and well-highlighted English summaries."""
        
        return system_prompt