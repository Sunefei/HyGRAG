import asyncio
import numpy as np
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict

from Core.Common.Logger import logger
from Core.Retriever.BaseRetriever import BaseRetriever
from Core.Retriever.RetrieverFactory import register_retriever_method
from Core.Common.Utils import truncate_list_by_token_size, min_max_normalize
from Core.Common.Constants import Retriever


class HKGraphTreeRetriever(BaseRetriever):
    """
    HKGraphTreeä¸“ç”¨æ£€ç´¢å™¨ï¼šä»é¡¶å‘ä¸‹æ£€ç´¢å±‚æ¬¡åŒ–ç¤¾åŒºã€å®ä½“ã€æ–‡æ¡£å—å’Œå…³ç³»
    
    æ ¸å¿ƒæ€è·¯ï¼š
    1. ä»é¡¶å±‚ç¤¾åŒºå¼€å§‹ï¼Œä½¿ç”¨æŸ¥è¯¢ç›¸ä¼¼æ€§ç­›é€‰ç›¸å…³ç¤¾åŒº
    2. é€å±‚å‘ä¸‹å±•å¼€ï¼Œè·å–å­ç¤¾åŒºå’Œæˆå‘˜èŠ‚ç‚¹
    3. åœ¨åº•å±‚æ”¶é›†ç›¸å…³çš„å®ä½“ã€æ–‡æ¡£å—å’Œå…³ç³»
    4. æ•´åˆå¤šå±‚æ¬¡ä¿¡æ¯å½¢æˆæœ€ç»ˆæ£€ç´¢ç»“æœ
    """
    
    def __init__(self, **kwargs):
        config = kwargs.pop("config")
        super().__init__(config)
        self.mode_list = ["hk_tree_search", "hk_tree_top_down", "hk_tree_comprehensive", "hk_tree_true_search", "hk_tree_flat_search"]
        self.type = "hk_tree"
        
        # è®¾ç½®å¿…éœ€çš„å±æ€§
        for key, value in kwargs.items():
            setattr(self, key, value)
        
        # Ensure key attributes exist
        if not hasattr(self, 'graph'):
            logger.warning("Graph attribute not found during HKGraphTreeRetriever initialization")
        if not hasattr(self, 'doc_chunk'):
            logger.warning("Doc_chunk attribute not found during HKGraphTreeRetriever initialization")


    async def _compute_community_similarity(self, query_embedding: np.ndarray, 
                                          community: Dict, summary: str) -> float:
        """
        è®¡ç®—æŸ¥è¯¢ä¸ç¤¾åŒºçš„ç›¸ä¼¼æ€§åˆ†æ•°
        ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„ node_text_embeddingsï¼Œé¿å…é‡å¤è®¡ç®—
        
        Args:
            query_embedding: æŸ¥è¯¢embedding
            community: ç¤¾åŒºä¿¡æ¯
            summary: ç¤¾åŒºæ‘˜è¦
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æ•°
        """
        try:
            community_id = community['id']
            
            # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„æ–‡æœ¬embeddingï¼ˆæœ€å¸¸è§ä¸”é«˜æ•ˆï¼‰
            if (hasattr(self.graph, 'node_text_embeddings') and 
                community_id in self.graph.node_text_embeddings):
                community_embedding = self.graph.node_text_embeddings[community_id]
                if isinstance(community_embedding, np.ndarray):
                    similarity = await self.compute_similarity(query_embedding, community_embedding, "cosine")
                    return float(similarity)
            
            
            
            # æ–¹æ³•3ï¼šåŸºäºç¤¾åŒºæ‘˜è¦é‡æ–°è®¡ç®—embeddingï¼ˆåº”è¯¥å¾ˆå°‘è§¦å‘ï¼‰
            if summary and summary.strip():
                logger.debug(f"âš ï¸ No cached embedding for community {community_id}, recalculating from summary...")
                summary_embedding = await self._get_query_embedding(summary)
                similarity = np.dot(query_embedding, summary_embedding)
                return float(similarity)
            
            # æ–¹æ³•4ï¼šå¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œè¿”å›ä½åˆ†
            logger.warning(f"No embedding or summary for community {community_id}")
            return 0.1
            
        except Exception as e:
            logger.warning(f"Failed to compute similarity for community {community.get('id', 'UNKNOWN')}: {e}")
            return 0.0

    async def _extract_entities_and_chunks_from_nodes(self, member_nodes: Set[str], 
                                                    query_embedding: np.ndarray) -> Tuple[List[Dict], List[Dict]]:
        """
        ä»æˆå‘˜èŠ‚ç‚¹ä¸­æå–å¹¶æ’åºå®ä½“å’Œæ–‡æ¡£å—
        
        Args:
            member_nodes: æˆå‘˜èŠ‚ç‚¹é›†åˆ
            query_embedding: æŸ¥è¯¢embedding
            
        Returns:
            (å®ä½“åˆ—è¡¨, æ–‡æ¡£å—åˆ—è¡¨)
        """
        entities = []
        chunks = []
        
        for node_id in member_nodes:
            if node_id.startswith('CHUNK_'):
                # å¤„ç†æ–‡æ¡£å—èŠ‚ç‚¹
                chunk_data = await self._get_chunk_data_with_similarity(node_id, query_embedding)
                if chunk_data:
                    chunks.append(chunk_data)
            elif not node_id.startswith('COMMUNITY_'):
                # å¤„ç†å®ä½“èŠ‚ç‚¹ï¼ˆè·³è¿‡ç¤¾åŒºèŠ‚ç‚¹ï¼‰
                entity_data = await self._get_entity_data_with_similarity(node_id, query_embedding)
                if entity_data:
                    entities.append(entity_data)
        
        # æŒ‰ç›¸ä¼¼æ€§åˆ†æ•°æ’åº
        entities.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        chunks.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        logger.debug(f"ğŸ“‹ Extracted and sorted {len(entities)} entities and {len(chunks)} chunks")
        return entities, chunks

    async def _get_chunk_data_with_similarity(self, chunk_node_id: str, query_embedding: np.ndarray) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£å—æ•°æ®å¹¶è®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°
        ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„ node_text_embeddingsï¼Œé¿å…é‡å¤è®¡ç®—
        """
        try:
            chunk_key = chunk_node_id.replace('CHUNK_', '')
            chunk_content = None
            
            if hasattr(self, 'doc_chunk') and self.doc_chunk is not None:
                # æ£€æŸ¥get_data_by_keyæ˜¯å¦æ˜¯asyncæ–¹æ³•
                try:
                    if hasattr(self.doc_chunk, 'get_data_by_key'):
                        # å°è¯•asyncè°ƒç”¨
                        if asyncio.iscoroutinefunction(self.doc_chunk.get_data_by_key):
                            chunk_content = await self.doc_chunk.get_data_by_key(chunk_key)
                        else:
                            # åŒæ­¥è°ƒç”¨
                            chunk_content = self.doc_chunk.get_data_by_key(chunk_key)
                    else:
                        logger.warning(f"doc_chunk does not have get_data_by_key method")
                        return None
                    
                    if chunk_content:
                        # ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„æ–‡æœ¬åµŒå…¥
                        if (hasattr(self.graph, 'node_text_embeddings') and 
                            chunk_node_id in self.graph.node_text_embeddings):
                            chunk_embedding = self.graph.node_text_embeddings[chunk_node_id]
                            if isinstance(chunk_embedding, np.ndarray):
                                similarity_score = await self.compute_similarity(query_embedding, chunk_embedding, "cosine")
                            else:
                                similarity_score = 0.0
                        else:
                            # å›é€€ï¼šé‡æ–°è®¡ç®— embedding
                            content_embedding = await self._get_query_embedding(chunk_content)
                            similarity_score = np.dot(query_embedding, content_embedding)
                        
                        return {
                            'id': chunk_key,
                            'content': chunk_content,
                            'type': 'chunk',
                            'similarity_score': float(similarity_score)
                        }
                except Exception as chunk_error:
                    logger.warning(f"Error accessing chunk data for {chunk_key}: {chunk_error}")
                    return None
        except Exception as e:
            logger.warning(f"Failed to get chunk {chunk_node_id} with similarity: {e}")
        return None

    async def _get_entity_data_with_similarity(self, entity_node_id: str, query_embedding: np.ndarray) -> Dict[str, Any]:
        """
        è·å–å®ä½“æ•°æ®å¹¶è®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°
        ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„ node_text_embeddingsï¼Œé¿å…é‡å¤è®¡ç®—
        """
        try:
            entity_data = await self.graph.get_node(entity_node_id)
            if entity_data:
                # ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„æ–‡æœ¬åµŒå…¥
                if (hasattr(self.graph, 'node_text_embeddings') and 
                    entity_node_id in self.graph.node_text_embeddings):
                    entity_embedding = self.graph.node_text_embeddings[entity_node_id]
                    if isinstance(entity_embedding, np.ndarray):
                        similarity_score = await self.compute_similarity(query_embedding, entity_embedding, "cosine")
                    else:
                        similarity_score = 0.0
                else:
                    # å›é€€ï¼šé‡æ–°è®¡ç®— embeddingï¼ˆåŸºäºåç§°å’Œæè¿°ï¼‰
                    entity_text = f"{entity_data.get('entity_name', '')} {entity_data.get('description', '')}"
                    if entity_text.strip():
                        entity_embedding = await self._get_query_embedding(entity_text)
                        similarity_score = np.dot(query_embedding, entity_embedding)
                    else:
                        similarity_score = 0.0
                
                return {
                    'entity_name': entity_data.get('entity_name', entity_node_id),
                    'entity_type': entity_data.get('entity_type', 'UNKNOWN'),
                    'description': entity_data.get('description', ''),
                    'type': 'entity',
                    'similarity_score': float(similarity_score)
                }
        except Exception as e:
            logger.warning(f"Failed to get entity {entity_node_id} with similarity: {e}")
        return None

    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """
        è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        """
        try:
            embedding = None
            
            # å°è¯•ä½¿ç”¨HKGraphTreeçš„embeddingæ–¹æ³•
            if hasattr(self.graph, '_embed_text'):
                try:
                    embedding = await self.graph._embed_text(query)
                    #logger.debug(f"Used graph._embed_text for query embedding")
                except Exception as e:
                    logger.warning(f"Failed to use graph._embed_text: {e}")
            
            # å›é€€åˆ°ç›´æ¥ä½¿ç”¨embedding_model
            if embedding is None and hasattr(self.graph, 'embedding_model'):
                try:
                    embedding = self.graph.embedding_model._get_text_embedding(query)
                    logger.debug(f"Used graph.embedding_model for query embedding")
                except Exception as e:
                    logger.warning(f"Failed to use graph.embedding_model: {e}")
            
            # å°è¯•ä½¿ç”¨entities_vdbçš„embeddingæ¨¡å‹
            if embedding is None and hasattr(self, 'entities_vdb') and hasattr(self.entities_vdb, 'embedding_model'):
                try:
                    embedding = self.entities_vdb.embedding_model._get_text_embedding(query)
                    logger.debug(f"Used entities_vdb.embedding_model for query embedding")
                except Exception as e:
                    logger.warning(f"Failed to use entities_vdb.embedding_model: {e}")
            
            # æœ€åçš„å›é€€ï¼šéšæœºåµŒå…¥
            if embedding is None:
                logger.warning("No embedding model found, using random embeddings")
                embedding = np.random.normal(0, 0.1, 128)
            
            # ç¡®ä¿embeddingæ˜¯numpyæ•°ç»„
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding)
            
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            else:
                # å¦‚æœnormä¸º0ï¼Œä½¿ç”¨éšæœºå‘é‡
                embedding = np.random.normal(0, 0.1, len(embedding))
                embedding = embedding / np.linalg.norm(embedding)
            
            return embedding
            
        except Exception as e:
            logger.error(f"Critical error in _get_query_embedding: {e}")
            logger.warning("Using fallback random embeddings")
            embedding = np.random.normal(0, 0.1, 128)
            return embedding / np.linalg.norm(embedding)

    async def _get_chunk_data(self, chunk_node_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£å—æ•°æ®
        """
        try:
            chunk_key = chunk_node_id.replace('CHUNK_', '')
            if hasattr(self, 'doc_chunk'):
                chunk_content = await self.doc_chunk.get_data_by_key(chunk_key)
                if chunk_content:
                    return {
                        'id': chunk_key,
                        'content': chunk_content,
                        'type': 'chunk'
                    }
        except Exception as e:
            logger.warning(f"Failed to get chunk {chunk_node_id}: {e}")
        return None

    async def _get_entity_data(self, entity_node_id: str) -> Dict[str, Any]:
        """
        è·å–å®ä½“æ•°æ®
        """
        try:
            entity_data = await self.graph.get_node(entity_node_id)
            if entity_data:
                return {
                    'entity_name': entity_data.get('entity_name', entity_node_id),
                    'entity_type': entity_data.get('entity_type', 'UNKNOWN'),
                    'description': entity_data.get('description', ''),
                    'type': 'entity'
                }
        except Exception as e:
            logger.warning(f"Failed to get entity {entity_node_id}: {e}")
        return None

    async def _get_relationships_between_nodes(self, node_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        è·å–èŠ‚ç‚¹é—´çš„å…³ç³» - åªè€ƒè™‘entityå’Œentityä¹‹é—´çš„å…³ç³»ï¼Œè·³è¿‡æ¶‰åŠchunkçš„å…³ç³»
        """
        relationships = []
        node_ids_list = list(node_ids)
        
        try:
            logger.debug(f"Getting entity-entity relationships for {len(node_ids_list)} nodes")
            
            for i, src_node in enumerate(node_ids_list):
                try:
                    # è·³è¿‡chunkèŠ‚ç‚¹å’ŒcommunityèŠ‚ç‚¹ï¼Œåªå¤„ç†entityèŠ‚ç‚¹
                    if src_node.startswith('CHUNK_') or src_node.startswith('COMMUNITY_'):
                        continue
                    
                    # æ£€æŸ¥graphæ˜¯å¦æœ‰get_node_edgesæ–¹æ³•
                    if not hasattr(self.graph, 'get_node_edges'):
                        logger.warning("Graph does not have get_node_edges method")
                        break
                    
                    # è·å–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰è¾¹
                    node_edges = await self.graph.get_node_edges(src_node)
                    if node_edges:
                        for edge_tuple in node_edges:
                            try:
                                if len(edge_tuple) >= 2:
                                    tgt_node = edge_tuple[1] if edge_tuple[0] == src_node else edge_tuple[0]
                                    
                                    # åªæœ‰å½“ç›®æ ‡èŠ‚ç‚¹ä¹Ÿæ˜¯entityï¼ˆä¸æ˜¯chunkæˆ–communityï¼‰ä¸”åœ¨èŠ‚ç‚¹é›†åˆä¸­æ—¶æ‰å¤„ç† #TODO 
                                    # if (tgt_node in node_ids and 
                                    #     not tgt_node.startswith('CHUNK_') and 
                                    #     not tgt_node.startswith('COMMUNITY_')):
                                    if (  not tgt_node.startswith('CHUNK_') and # åªæœ‰å½“ç›®æ ‡èŠ‚ç‚¹ä¹Ÿæ˜¯entityï¼ˆä¸æ˜¯chunkæˆ–communityï¼‰
                                        not tgt_node.startswith('COMMUNITY_')):
                                        
                                        # è·å–è¾¹æ•°æ®
                                        edge_data = await self.graph.get_edge(src_node, tgt_node)
                                        if edge_data:
                                            relationships.append({
                                                'src_id': edge_data.get('src_id', src_node),
                                                'tgt_id': edge_data.get('tgt_id', tgt_node),
                                                'relation_name': edge_data.get('relation_name', 'CONNECTED'),
                                                'description': edge_data.get('description', ''),
                                                'type': 'relationship'
                                            })
                            except Exception as edge_error:
                                logger.warning(f"Error processing edge {edge_tuple}: {edge_error}")
                                continue
                except Exception as node_error:
                    logger.warning(f"Error getting edges for node {src_node}: {node_error}")
                    continue
                    
        except Exception as e:
            logger.error(f"Critical error in _get_relationships_between_nodes: {e}")
        
        logger.debug(f"Found {len(relationships)} entity-entity relationships (skipped chunk-related edges)")
        return relationships

    async def _compute_text_similarity(self, query_embedding: np.ndarray, text: str) -> float:
        """
        è®¡ç®—æŸ¥è¯¢åµŒå…¥ä¸æ–‡æœ¬çš„ç›¸ä¼¼æ€§
        """
        try:
            if not text.strip():
                return 0.0
            
            # è·å–æ–‡æœ¬åµŒå…¥
            text_embedding = await self._get_query_embedding(text)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
            similarity = np.dot(query_embedding, text_embedding)
            return float(similarity)
        except Exception as e:
            logger.warning(f"Failed to compute text similarity: {e}")
            return 0.0

    async def _fallback_retrieval(self, query: str, seed_entities: List[Dict]) -> Dict[str, Any]:
        """
        å½“å±‚æ¬¡åŒ–æ£€ç´¢ä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ³•
        """
        logger.info("ğŸ”„ Using fallback retrieval method...")
        
        results = {
            'entities': [],
            'chunks': [],
            'relationships': [],
            'communities': [],
            'community_summaries': []
        }
        
        try:
            # å°è¯•åŸºç¡€çš„å®ä½“æ£€ç´¢
            if hasattr(self, 'entities_vdb') and seed_entities:
                entity_results = []
                for seed_entity in seed_entities[:self.config.top_k]:
                    entity_results.append({
                        'entity_name': seed_entity.get('entity_name', 'UNKNOWN'),
                        'entity_type': seed_entity.get('entity_type', 'UNKNOWN'),
                        'description': seed_entity.get('description', ''),
                        'type': 'entity'
                    })
                results['entities'] = entity_results
            
            # å°è¯•åŸºç¡€çš„æ–‡æ¡£å—æ£€ç´¢
            if hasattr(self, 'doc_chunk'):
                # è·å–ä¸€äº›ç¤ºä¾‹æ–‡æ¡£å—
                try:
                    sample_chunks = []
                    for i in range(min(self.config.top_k, 5)):
                        chunk_data = await self.doc_chunk.get_data_by_index(i)
                        if chunk_data:
                            sample_chunks.append({
                                'id': str(i),
                                'content': chunk_data,
                                'type': 'chunk'
                            })
                    results['chunks'] = sample_chunks
                except:
                    pass
            
        except Exception as e:
            logger.error(f"Fallback retrieval failed: {e}")
        
        return results

    async def _seed_entity_based_retrieval(self, query: str, seed_entities: List[Dict]) -> Dict[str, Any]:
        """
        åŸºäºç§å­å®ä½“çš„æ£€ç´¢
        """
        # è¿™é‡Œå¯ä»¥å®ç°åŸºäºç§å­å®ä½“çš„ä¼ ç»Ÿæ£€ç´¢é€»è¾‘
        # ä½œä¸ºå±‚æ¬¡åŒ–æ£€ç´¢çš„è¡¥å……
        return await self._fallback_retrieval(query, seed_entities)

    ### 
    @register_retriever_method(type="hk_tree", method_name="hk_tree_flat_search")
    async def _hk_tree_flat_search_retrieval(self, query: str, seed_entities: List[Dict] = None, **kwargs) -> Dict[str, Any]:
        """
        æ‰å¹³åŒ–æ£€ç´¢ï¼šç›´æ¥åœ¨æ‰€æœ‰å±‚æ¬¡çš„æ‰€æœ‰èŠ‚ç‚¹ä¸­å¯»æ‰¾top_kä¸ªæœ€ç›¸å…³çš„èŠ‚ç‚¹
        ä¸è€ƒè™‘å±‚æ¬¡ç»“æ„ï¼Œå°†æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŸºç¡€èŠ‚ç‚¹+ç¤¾åŒºèŠ‚ç‚¹ï¼‰æ‰å¹³åŒ–å¤„ç†
        """
        #logger.info("ğŸ” Starting flat search across all hierarchy levels...")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒå±‚æ¬¡ç»“æ„
        if not hasattr(self.graph, 'get_hierarchy_info'):
            logger.warning("Graph does not support hierarchy structure, falling back")
            return await self._fallback_retrieval(query, seed_entities)
        
        hierarchy_info = await self.graph.get_hierarchy_info()
        if not hierarchy_info or hierarchy_info.get('levels', 0) == 0:
            logger.warning("No hierarchy found, falling back")
            return await self._fallback_retrieval(query, seed_entities)
        
        try:
            # Step 1: è·å–æŸ¥è¯¢åµŒå…¥
            query_embedding = await self._get_query_embedding(query)
            
            # Step 2: ä½¿ç”¨FAISSæ£€ç´¢æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹
            top_k = getattr(self.config, 'top_k', 5)
            extended_top_k = min(top_k * 200, 1000)  
            
            top_nodes = await self._faiss_search_top_nodes(query_embedding, top_k)
            #top_nodes = []
            
            if not top_nodes:
                logger.warning("No nodes found from FAISS search, falling back to manual search")
                # fallback
                all_nodes_with_scores = await self._collect_all_nodes_with_similarity(query_embedding, hierarchy_info)
                if not all_nodes_with_scores:
                    return await self._fallback_retrieval(query, seed_entities)
                sorted_nodes = sorted(all_nodes_with_scores, key=lambda x: x['similarity_score'], reverse=True)
                top_nodes = sorted_nodes[:extended_top_k]
            
            #logger.info(f"ğŸ“Š From {len(all_nodes_with_scores)} total nodes, selected top {len(top_nodes)} for processing")
            
            # Step 4: åˆ†ç±»å¹¶æ„å»ºæœ€ç»ˆç»“æœ
            results = await self._build_flat_search_results(top_nodes, query_embedding, top_k)
            
            # logger.info(f"ğŸ¯ Flat search completed: {len(results.get('communities', []))} communities, "
            #            f"{len(results.get('entities', []))} entities, {len(results.get('chunks', []))} chunks")
            
            return results
            
        except Exception as e:
            logger.error(f"Flat search failed: {e}")
            return await self._fallback_retrieval(query, seed_entities)
    
    async def _collect_all_nodes_with_similarity(self, query_embedding: np.ndarray, hierarchy_info: Dict) -> List[Dict]:
        """
        æ”¶é›†æ‰€æœ‰å±‚æ¬¡çš„æ‰€æœ‰èŠ‚ç‚¹å¹¶è®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°
        """
        all_nodes_with_scores = []
        
        # 1. æ”¶é›†æ‰€æœ‰åŸºç¡€èŠ‚ç‚¹ï¼ˆentitieså’Œchunksï¼‰
        base_nodes = []
        if hasattr(self.graph, 'node_embeddings'):
            for node_id in self.graph.node_embeddings.keys():
                if not node_id.startswith('COMMUNITY_'):
                    base_nodes.append(node_id)
        
        #logger.info(f"ğŸ“‹ Found {len(base_nodes)} base nodes (entities + chunks)")
        
        # è®¡ç®—åŸºç¡€èŠ‚ç‚¹çš„ç›¸ä¼¼æ€§
        for node_id in base_nodes:
            try:
                similarity_score = await self._compute_node_similarity(query_embedding, node_id)
                
                node_info = {
                    'id': node_id,
                    'type': 'chunk' if node_id.startswith('CHUNK_') else 'entity',
                    'level': 0,  # åŸºç¡€å±‚
                    'similarity_score': similarity_score
                }
                all_nodes_with_scores.append(node_info)
                
            except Exception as e:
                logger.warning(f"Failed to compute similarity for base node {node_id}: {e}")
                continue
        
        # 2. æ”¶é›†æ‰€æœ‰ç¤¾åŒºèŠ‚ç‚¹
        hierarchy_levels = hierarchy_info.get('hierarchy_levels', {})
        community_summaries = hierarchy_info.get('community_summaries', {})
        
        community_count = 0
        for level, communities in hierarchy_levels.items():
            for community in communities:
                community_id = community['id']
                try:
                    # è®¡ç®—ç¤¾åŒºç›¸ä¼¼æ€§
                    similarity_score = await self._compute_community_similarity(
                        query_embedding, community, community_summaries.get(community_id, '')
                    )
                    
                    community_info = {
                        'id': community_id,
                        'type': 'community',
                        'level': level,
                        'similarity_score': similarity_score,
                        'member_count': len(community.get('nodes', [])),
                        'summary': community_summaries.get(community_id, '')
                    }
                    all_nodes_with_scores.append(community_info)
                    community_count += 1
                    
                except Exception as e:
                    logger.warning(f"Failed to compute similarity for community {community_id}: {e}")
                    continue
        
        #logger.info(f"ğŸ“‹ Found {community_count} community nodes across all levels")
        #logger.info(f"ğŸ“Š Total nodes for flat search: {len(all_nodes_with_scores)}")
        
        return all_nodes_with_scores
    
    async def _compute_node_similarity(self, query_embedding: np.ndarray, node_id: str) -> float:
        """
        è®¡ç®—åŸºç¡€èŠ‚ç‚¹ï¼ˆentityæˆ–chunkï¼‰ä¸æŸ¥è¯¢çš„ç›¸ä¼¼æ€§
        ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„ node_text_embeddingsï¼Œé¿å…é‡å¤è®¡ç®—
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„æ–‡æœ¬åµŒå…¥ï¼ˆè¿™æ˜¯æœ€å¸¸è§å’Œé«˜æ•ˆçš„æ–¹å¼ï¼‰
            if (hasattr(self.graph, 'node_text_embeddings') and 
                node_id in self.graph.node_text_embeddings):
                node_embedding = self.graph.node_text_embeddings[node_id]
                if isinstance(node_embedding, np.ndarray):
                    similarity = await self.compute_similarity(query_embedding, node_embedding, "cosine")
                    return float(similarity)
            
            # å›é€€æ–¹æ¡ˆï¼šåŸºäºæ–‡æœ¬å†…å®¹é‡æ–°è®¡ç®—ï¼ˆåº”è¯¥å¾ˆå°‘è§¦å‘ï¼‰
            logger.debug(f"âš ï¸ No cached embedding for {node_id}, recalculating...")
            
            if node_id.startswith('CHUNK_'):
                # ChunkèŠ‚ç‚¹ï¼šä½¿ç”¨chunkå†…å®¹
                chunk_key = node_id.replace('CHUNK_', '')
                chunk_content = await self._get_chunk_content_for_similarity(chunk_key)
                if chunk_content:
                    content_embedding = await self._get_query_embedding(chunk_content)
                    similarity = np.dot(query_embedding, content_embedding)
                    return float(similarity)
            else:
                # EntityèŠ‚ç‚¹ï¼šä½¿ç”¨å®ä½“åç§°å’Œæè¿°
                entity_data = await self.graph.get_node(node_id)
                if entity_data:
                    entity_text = f"{entity_data.get('entity_name', '')} {entity_data.get('description', '')}"
                    if entity_text.strip():
                        entity_embedding = await self._get_query_embedding(entity_text)
                        similarity = np.dot(query_embedding, entity_embedding)
                        return float(similarity)
            
            return 0.0
            
        except Exception as e:
            logger.warning(f"Failed to compute similarity for node {node_id}: {e}")
            return 0.0

    async def compute_similarity(self, query_embedding: np.ndarray, 
                       node_embedding: np.ndarray, 
                       method: str = "cosine") -> float:
        """
        è®¡ç®— query å’Œ node çš„ç›¸ä¼¼åº¦
        
        å‚æ•°:
            query_embedding: np.ndarray, æŸ¥è¯¢å‘é‡
            node_embedding: np.ndarray, èŠ‚ç‚¹å‘é‡
            method: str, "cosine" æˆ– "l2"
        
        è¿”å›:
            similarity: float, ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not isinstance(query_embedding, np.ndarray) or not isinstance(node_embedding, np.ndarray):
            raise ValueError("Both embeddings must be numpy arrays.")

        if method == "cosine":
            # å½’ä¸€åŒ–åç‚¹ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦
            q_norm = query_embedding / np.linalg.norm(query_embedding)
            n_norm = node_embedding / np.linalg.norm(node_embedding)
            similarity = np.dot(q_norm, n_norm)
            return float(similarity)

        elif method == "l2":
            # L2 è·ç¦» -> ç›¸ä¼¼åº¦ (å€¼è¶Šå¤§è¶Šç›¸ä¼¼)
            distance = np.linalg.norm(query_embedding - node_embedding)
            similarity = 1 / (1 + distance)
            return float(similarity)

        else:
            raise ValueError(f"Unknown method '{method}'. Use 'cosine' or 'l2'.")
    
    async def _get_chunk_content_for_similarity(self, chunk_key: str) -> str:
        """
        è·å–chunkå†…å®¹ç”¨äºç›¸ä¼¼æ€§è®¡ç®—
        """
        try:
            if hasattr(self, 'doc_chunk') and self.doc_chunk is not None:
                if hasattr(self.doc_chunk, 'get_data_by_key'):
                    if asyncio.iscoroutinefunction(self.doc_chunk.get_data_by_key):
                        return await self.doc_chunk.get_data_by_key(chunk_key)
                    else:
                        return self.doc_chunk.get_data_by_key(chunk_key)
        except Exception as e:
            logger.warning(f"Failed to get chunk content for {chunk_key}: {e}")
        return ""
    
    async def _build_flat_search_results(self, top_nodes: List[Dict], query_embedding: np.ndarray, top_k: int) -> Dict[str, Any]:
        """
        ä»æ‰å¹³åŒ–æœç´¢çš„topèŠ‚ç‚¹æ„å»ºæœ€ç»ˆç»“æœ
        """
        results = {
            'communities': [],
            'entities': [],
            'chunks': [],
            'relationships': [],
            'community_summaries': []
        }
        
        # åˆ†ç±»å¤„ç†topèŠ‚ç‚¹
        selected_communities = []
        selected_entities = []  # æ¥è‡ªtop_nodesçš„å®ä½“ï¼Œå—temp_entity_limité™åˆ¶
        selected_chunks = []
        
        # æŒ‰ç±»å‹å’Œç›¸ä¼¼æ€§åˆ†é…èŠ‚ç‚¹ #TODO
        # community + chunk = top_k, entity = top_k, relationships = top_k
        max_community_limit = max(1, top_k // 5)  # æœ€å¤§ç¤¾åŒºæ•°é‡ä¸º1ã€1/5 
        community_limit = max_community_limit #TODO debug
        chunk_limit = top_k - max_community_limit  # å‰©ä½™å…¨éƒ¨ç»™chunks
        entity_limit = top_k  # å®ä½“æ•°é‡ä¸ºtop_k
        temp_entity_limit = entity_limit * 3 #TODO relationçš„entity
        relationship_limit = top_k  # å…³ç³»æ•°é‡ä¸ºtop_k
        
        for node in top_nodes:
            node_type = node['type']
            node_id = node['id']
            # Old method 1 community and 4 chunks
            # if node_type == 'community' and len(selected_communities) < community_limit:
            #     # æ·»åŠ ç¤¾åŒºä¿¡æ¯
            #     community_info = {
            #         'id': node_id,
            #         'level': node['level'],
            #         'member_count': node.get('member_count', 0),
            #         'summary': node.get('summary', ''),
            #         'similarity_score': node['similarity_score']
            #     }
            #     selected_communities.append(community_info)
            #     results['community_summaries'].append(community_info['summary'])
                
            #     # æ”¶é›†ç¤¾åŒºæˆå‘˜èŠ‚ç‚¹
            #     if hasattr(self.graph, 'get_community_children'):
            #         try:
            #             children = await self.graph.get_community_children(node_id)
            #             all_member_nodes.update(children)
            #         except Exception as e:
            #             logger.warning(f"Failed to get children for community {node_id}: {e}")
                        
            # elif node_type == 'entity' and len(selected_entities) < temp_entity_limit: #åŸæœ¬ä¸ºentity_limit
            #     # æ·»åŠ å®ä½“ä¿¡æ¯
            #     entity_data = await self._get_entity_data_with_similarity(node_id, query_embedding)
            #     if entity_data:
            #         selected_entities.append(entity_data)
            #         all_member_nodes.add(node_id)
                    
            # elif node_type == 'chunk' and len(selected_chunks) < chunk_limit:
            #     # æ·»åŠ chunkä¿¡æ¯
            #     chunk_data = await self._get_chunk_data_with_similarity(node_id, query_embedding)
            #     if chunk_data:
            #         selected_chunks.append(chunk_data)
            #         all_member_nodes.add(node_id)

            # New method Free Community and chunks
                        
            if node_type == 'entity' and len(selected_entities) < temp_entity_limit: #åŸæœ¬ä¸ºentity_limit
                # æ·»åŠ å®ä½“ä¿¡æ¯
                entity_data = await self._get_entity_data_with_similarity(node_id, query_embedding)
                if entity_data:
                    selected_entities.append(entity_data)

            elif (node_type == 'community' or node_type == 'chunk') and len(selected_communities) + len(selected_chunks)< top_k:
                # æ·»åŠ ç¤¾åŒºä¿¡æ¯
                if node_type == 'community' and len(selected_communities) < max_community_limit:
                    community_info = {
                        'id': node_id,
                        'level': node['level'],
                        'member_count': node.get('member_count', 0),
                        'summary': node.get('summary', ''),
                        'similarity_score': node['similarity_score']
                    }
                    selected_communities.append(community_info)
                    results['community_summaries'].append(community_info['summary'])
                    
                elif node_type == 'chunk' and len(selected_chunks) < top_k:
                    # æ·»åŠ chunkä¿¡æ¯
                    chunk_data = await self._get_chunk_data_with_similarity(node_id, query_embedding)
                    if chunk_data:
                        selected_chunks.append(chunk_data)
            
        # ç»§ç»­æ”¶é›†entityç›´åˆ°è¾¾åˆ°entity_limit
        for node in top_nodes:
            if len(selected_entities) >= temp_entity_limit:
                break
                
            node_type = node['type']
            node_id = node['id']
            
            if node_type == 'entity':
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªentity
                already_added = any(e.get('entity_name') == node_id for e in selected_entities)
                
                if not already_added:
                    entity_data = await self._get_entity_data_with_similarity(node_id, query_embedding)
                    if entity_data:
                        selected_entities.append(entity_data)
        
        # å¦‚æœcommunity + chunkæ€»æ•°ä¸å¤Ÿtop_kï¼Œä¼˜å…ˆè¡¥å……chunk
        community_chunk_total = len(selected_communities) + len(selected_chunks)
        if community_chunk_total < top_k:
            remaining_slots = top_k - community_chunk_total
            for node in top_nodes:
                if remaining_slots <= 0:
                    break
                    
                node_type = node['type']
                node_id = node['id']
                
                if node_type == 'chunk':
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªchunk
                    already_added = any(c.get('id') == node_id.replace('CHUNK_', '') for c in selected_chunks)
                    
                    if not already_added:
                        chunk_data = await self._get_chunk_data_with_similarity(node_id, query_embedding)
                        if chunk_data:
                            selected_chunks.append(chunk_data)
                            remaining_slots -= 1
        
        # ä»é€‰ä¸­çš„ç¤¾åŒºä¸­æå–å®ä½“ï¼ˆä¸å—temp_entity_limité™åˆ¶ï¼‰
        community_entities = []
        if selected_communities:
            logger.debug(f"ğŸ˜ï¸ Extracting entities from {len(selected_communities)} selected communities")
            
            for community_info in selected_communities:
                community_id = community_info['id']
                try:
                    # è·å–ç¤¾åŒºçš„å­èŠ‚ç‚¹
                    if hasattr(self.graph, 'get_community_children'):
                        children = await self.graph.get_community_children(community_id)
                        
                        for child_node_id in children:
                            # åªå¤„ç†å®ä½“èŠ‚ç‚¹ï¼ˆè·³è¿‡chunkå’ŒcommunityèŠ‚ç‚¹ï¼‰
                            if (not child_node_id.startswith('CHUNK_') and 
                                not child_node_id.startswith('COMMUNITY_')):
                                
                                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨selected_entitiesä¸­
                                already_selected = any(e.get('entity_name') == child_node_id for e in selected_entities)
                                
                                if not already_selected:
                                    entity_data = await self._get_entity_data_with_similarity(child_node_id, query_embedding)
                                    if entity_data:
                                        # æ ‡è®°è¿™æ˜¯æ¥è‡ªç¤¾åŒºçš„å®ä½“
                                        entity_data['source'] = 'community_member'
                                        community_entities.append(entity_data)
                                        
                except Exception as e:
                    logger.warning(f"Failed to extract entities from community {community_id}: {e}")
            
            logger.debug(f"âœ… Extracted {len(community_entities)} entities from communities")
        
        # åˆå¹¶ä¸¤ç§æ¥æºçš„å®ä½“ç”¨äºå…³ç³»æŸ¥æ‰¾ï¼ˆå»é‡ï¼‰
        all_entities_for_relations = selected_entities.copy()  # æ¥è‡ªtop_nodesçš„å®ä½“
        #community_entities = [] #TODO ablation
        # æ·»åŠ æ¥è‡ªç¤¾åŒºçš„å®ä½“ï¼Œå»é‡
        for community_entity in community_entities:
            entity_name = community_entity.get('entity_name', '')
            already_exists = any(e.get('entity_name') == entity_name for e in all_entities_for_relations)
            if not already_exists:
                all_entities_for_relations.append(community_entity)
        
        # æŒ‰ç›¸ä¼¼æ€§æ’åºç»“æœ
        selected_entities.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        community_entities.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        all_entities_for_relations.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        selected_chunks.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        selected_communities.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        # å¡«å……ç»“æœ - æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„æ•°é‡åˆ†é…
        # community + chunk = top_k, entity = top_k, relationships = top_k
        results['communities'] = selected_communities
        results['entities'] = selected_entities[:entity_limit]  # åªè¾“å‡ºæ¥è‡ªtop_nodesçš„å®ä½“ï¼Œé™åˆ¶ä¸ºentity_limit
        results['chunks'] = selected_chunks

        # ç”¨äºå…³ç³»è®¡ç®—çš„å®ä½“ï¼ˆåŒ…å«ä¸¤ç§æ¥æºçš„å®ä½“ï¼‰
        all_entities_for_relations = all_entities_for_relations[:entity_limit*5] #TODO debug
        selected_entity_names = [d['entity_name'] for d in all_entities_for_relations]
        relationships = await self._get_relationships_between_nodes(selected_entity_names) #ä»åˆå¹¶åçš„å®ä½“ä¸­è·å–å…³ç³»
        selected_relationships = await self._get_select_relationships_from_relationships(relationships, query_embedding, top_k)
        results['relationships'] = selected_relationships[:relationship_limit]

        
        # ç¡®ä¿ community + chunk çš„æ€»æ•°ä¸è¶…è¿‡ top_k
        total_community_chunk = len(results['communities']) + len(results['chunks'])
        if total_community_chunk > top_k:
            # å¦‚æœè¶…è¿‡äº†ï¼Œä¼˜å…ˆä¿ç•™communityï¼Œç„¶åè°ƒæ•´chunkæ•°é‡
            max_chunks = top_k - len(results['communities'])
            results['chunks'] = results['chunks'][:max_chunks]
        
        logger.info(f"ğŸ“‹ Flat search results: {len(results['communities'])} communities, "
                   f"{len(results['entities'])} entities (from top_nodes), {len(results['chunks'])} chunks, "
                   f"{len(results['relationships'])} relationships")
        logger.info(f"ğŸ”— Relation calculation used {len(all_entities_for_relations)} total entities "
                   f"({len(selected_entities)} from top_nodes + {len(community_entities)} from communities)")
        # logger.info(f"ğŸ“Š Distribution: communities+chunks={len(results['communities'])+len(results['chunks'])}/{top_k}, "
        #            f"entities={len(results['entities'])}/{top_k}, relationships={len(results['relationships'])}/{top_k}")
        
        return results

    async def _get_select_relationships_from_relationships(self, relationships: List[Dict], query_embedding: np.ndarray, top_k: int) -> List[Dict]:
        """
        ä»å…³ç³»ä¸­é€‰æ‹©top_kä¸ªæœ€ç›¸å…³çš„å…³ç³»
        
        Args:
            relationships: å€™é€‰å…³ç³»åˆ—è¡¨
            query_embedding: æŸ¥è¯¢çš„embeddingå‘é‡
            top_k: é€‰æ‹©çš„å…³ç³»æ•°é‡
            
        Returns:
            æŒ‰ç›¸ä¼¼æ€§æ’åºçš„top_kä¸ªå…³ç³»åˆ—è¡¨
        """
        if not relationships:
            return []
        
        # è®¡ç®—æ¯ä¸ªå…³ç³»ä¸æŸ¥è¯¢çš„ç›¸ä¼¼æ€§åˆ†æ•°
        relationship_scores = []
        
        for relationship in relationships:
            try:
                # æ„å»ºå…³ç³»çš„æ–‡æœ¬è¡¨ç¤ºï¼šç»„åˆsrc_id, relation_name, tgt_idå’Œdescription
                relation_text_parts = []
                
                # æ·»åŠ æºå®ä½“å’Œç›®æ ‡å®ä½“
                src_id = relationship.get('src_id', '')
                tgt_id = relationship.get('tgt_id', '')
                relation_name = relationship.get('relation_name', '')
                description = relationship.get('description', '')
                
                # æ„å»ºå…³ç³»æ–‡æœ¬
                if src_id and relation_name and tgt_id:
                    relation_text_parts.append(f"{src_id} {relation_name} {tgt_id}")
                
                # æ·»åŠ æè¿°ä¿¡æ¯
                if description and description.strip():
                    relation_text_parts.append(description.strip())
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                if not relation_text_parts:
                    relation_text_parts.append(f"{src_id} connected to {tgt_id}")
                
                # ç»„åˆæ‰€æœ‰æ–‡æœ¬
                relation_text = ". ".join(relation_text_parts)
                
                # è®¡ç®—å…³ç³»æ–‡æœ¬çš„embedding
                relation_embedding = await self._get_query_embedding(relation_text)
                
                # è®¡ç®—ä¸æŸ¥è¯¢çš„ä½™å¼¦ç›¸ä¼¼æ€§
                similarity_score = np.dot(query_embedding, relation_embedding) #TODO å…³ç³»ç›¸ä¼¼åº¦é€‰æ‹©
                
                # åˆ›å»ºå¸¦åˆ†æ•°çš„å…³ç³»å‰¯æœ¬
                scored_relationship = relationship.copy()
                scored_relationship['similarity_score'] = float(similarity_score)
                scored_relationship['relation_text'] = relation_text  # ä¿å­˜ç”¨äºè°ƒè¯•
                
                relationship_scores.append(scored_relationship)
                
            except Exception as e:
                logger.warning(f"Failed to compute similarity for relationship {relationship}: {e}")
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œèµ‹äºˆä½åˆ†æ•°ä½†ä»ä¿ç•™
                fallback_relationship = relationship.copy()
                fallback_relationship['similarity_score'] = 0.0
                relationship_scores.append(fallback_relationship)
        
        # æŒ‰ç›¸ä¼¼æ€§åˆ†æ•°é™åºæ’åº
        relationship_scores.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # å»é‡ï¼šç”¨ (src_id, relation_name, tgt_id) ä½œä¸ºå”¯ä¸€é”®
        seen = set()
        unique_relationships = []
        for rel in relationship_scores:
            key = (rel.get('src_id', ''), rel.get('relation_name', ''), rel.get('tgt_id', ''))
            if key not in seen:
                seen.add(key)
                unique_relationships.append(rel)

        # é€‰æ‹©top_kä¸ªå…³ç³»
        selected_relationships = unique_relationships[:top_k]
        
        logger.debug(f"ğŸ”— Selected {len(selected_relationships)} relationships from {len(relationships)} candidates")
        
        # å¯é€‰ï¼šè®°å½•topå…³ç³»çš„ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        # for i, rel in enumerate(selected_relationships[:3]):  # åªè®°å½•å‰3ä¸ª
        #     logger.debug(f"   Top {i+1}: {rel.get('src_id', '')} -> {rel.get('tgt_id', '')} "
        #                  f"(relation: {rel.get('relation_name', '')}, score: {rel.get('similarity_score', 0):.3f})")
        
        return selected_relationships

    async def _faiss_search_top_nodes(self, query_embedding: np.ndarray, top_k: int) -> List[Dict]:
        """
        ä½¿ç”¨FAISSæ£€ç´¢æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹ï¼Œæ¯ç§ç±»å‹éƒ½æ£€ç´¢top_kä¸ª
        
        Args:
            query_embedding: æŸ¥è¯¢çš„embeddingå‘é‡
            top_k: æ¯ç§ç±»å‹è¿”å›çš„èŠ‚ç‚¹æ•°é‡
            
        Returns:
            æŒ‰ç›¸ä¼¼æ€§æ’åºçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            # æ£€æŸ¥graphæ˜¯å¦æœ‰ä¸“ç”¨çš„FAISSæœç´¢åŠŸèƒ½
            if not hasattr(self.graph, 'search_similar_entities'):
                logger.warning("Graph does not support specialized FAISS search methods")
                return []
            
            logger.info(f"ğŸ” Using FAISS to search for top {top_k} nodes of each type (chunk, entity, community)")
            
            all_faiss_results = []
            
            # åˆ†åˆ«ä»ä¸‰ç§ä¸“ç”¨ç´¢å¼•ä¸­æœç´¢ï¼Œæ¯ç§ç±»å‹éƒ½æœç´¢top_kä¸ª
            node_types = ['chunk', 'entity', 'community']
            
            for node_type in node_types:
                logger.debug(f"ğŸ” Searching {node_type} index for top {top_k} nodes")
                
                try:
                    # ä½¿ç”¨ä¸“ç”¨çš„æœç´¢æ–¹æ³•
                    if node_type == 'chunk':
                        current_results = await self.graph.search_similar_chunks(
                            query_embedding=query_embedding,
                            top_k=top_k 
                        )
                    elif node_type == 'entity':
                        current_results = await self.graph.search_similar_entities(
                            query_embedding=query_embedding,
                            top_k=top_k*5
                        )
                    elif node_type == 'community':
                        current_results = await self.graph.search_similar_communities(
                            query_embedding=query_embedding,
                            top_k=top_k 
                        )
                    else:
                        continue
                    
                    if current_results:
                        all_faiss_results.extend(current_results)
                        logger.debug(f"âœ… Found {len(current_results)} {node_type} nodes")
                    else:
                        logger.debug(f"âš ï¸ No results found for {node_type}")
                        
                except Exception as e:
                    logger.warning(f"Error searching {node_type} index: {e}")
                    continue
            
            if not all_faiss_results:
                logger.warning("FAISS search returned no results from any index")
                return []
            
            # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
            all_faiss_results.sort(key=lambda x: x[1], reverse=True)
            faiss_results = all_faiss_results

            if not faiss_results:
                logger.warning("FAISS search returned no results")
                return []
            
            # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
            node_ids = [node_id for node_id, score in faiss_results]
            node_info_dict = await self.graph.get_node_info_by_ids(node_ids)
            
            # è½¬æ¢ä¸ºä¸åŸæ ¼å¼å…¼å®¹çš„ç»“æ„
            top_nodes = []
            for node_id, similarity_score in faiss_results:
                node_info = node_info_dict.get(node_id, {})
                node_type = node_info.get('node_type', 'unknown')
                
                # è½¬æ¢èŠ‚ç‚¹ç±»å‹åç§°ä»¥ä¿æŒå…¼å®¹æ€§
                if node_type == 'chunk':
                    formatted_type = 'chunk'
                elif node_type == 'entity':
                    formatted_type = 'entity'
                elif node_type == 'community':
                    formatted_type = 'community'
                else:
                    formatted_type = 'entity'  # é»˜è®¤ä¸ºentity
                
                # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯
                node_data = {
                    'id': node_id,
                    'type': formatted_type,
                    'similarity_score': float(similarity_score)
                }
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                if formatted_type == 'community':
                    node_data['level'] = node_info.get('level', 0)
                    node_data['member_count'] = len(node_info.get('children', []))
                    node_data['summary'] = node_info.get('content', '')
                elif formatted_type == 'chunk':
                    node_data['content'] = node_info.get('content', '')
                elif formatted_type == 'entity':
                    node_data['entity_name'] = node_info.get('entity_name', node_id)
                    node_data['description'] = node_info.get('description', '')
                
                top_nodes.append(node_data)
            
            chunk_count = sum(1 for n in top_nodes if n['type'] == 'chunk')
            entity_count = sum(1 for n in top_nodes if n['type'] == 'entity')  
            community_count = sum(1 for n in top_nodes if n['type'] == 'community')
            
            logger.info(f"âœ… FAISS multi-index search found {len(top_nodes)} nodes (top {top_k} of each type): "
                       f"{chunk_count} chunks, {entity_count} entities, {community_count} communities")
            
            return top_nodes
            
        except Exception as e:
            logger.error(f"âŒ FAISS search failed: {e}")
            logger.exception(e)
            return []