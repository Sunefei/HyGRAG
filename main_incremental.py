"""
HKGraphTreeå¢é‡æ›´æ–°æµ‹è¯•ç¨‹åº

æµ‹è¯•EraRAG TreeGraphDynamicå¢é‡æ›´æ–°ç®—æ³•åœ¨HKGraphTreeä¸­çš„é›†æˆæ•ˆæœ

ä½¿ç”¨æ–¹æ³•:
1. åˆå§‹æ„å»º: python main_incremental.py -opt Option/Ours/HKGraphTreeDynamic.yaml -dataset_name multihop-rag -mode build
2. å¢é‡æ›´æ–°: python main_incremental.py -opt Option/Ours/HKGraphTreeDynamic.yaml -dataset_name multihop-rag -mode incremental
3. æ€§èƒ½æµ‹è¯•: python main_incremental.py -opt Option/Ours/HKGraphTreeDynamic.yaml -dataset_name multihop-rag -mode benchmark
"""

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import warnings
warnings.filterwarnings("ignore")
import nltk
nltk.data.path.append('/data/zhy/nltk_data')

from Core.GraphRAG import GraphRAG
from Option.Config2 import Config
import argparse
import asyncio
from pathlib import Path
from shutil import copyfile
from Data.QueryDataset import RAGQueryDataset
import pandas as pd
from Core.Utils.Evaluation import Evaluator
import time
import json
from Core.Common.Logger import logger


def parse_args():
    parser = argparse.ArgumentParser(description="HKGraphTreeå¢é‡æ›´æ–°æµ‹è¯•ç¨‹åº")
    parser.add_argument("-opt", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-dataset_name", type=str, required=True, help="æ•°æ®é›†åç§°")
    parser.add_argument("-mode", type=str, choices=["build", "incremental", "benchmark", "query"], 
                       default="build", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("-incremental_ratio", type=float, default=0.2, 
                       help="å¢é‡æ›´æ–°çš„æ•°æ®æ¯”ä¾‹ (0.1 = 10%)")
    parser.add_argument("-root", type=str, default="", help="ç»“æœç›®å½•å‰ç¼€")
    parser.add_argument("-enable_query", type=str, default="1", help="æ˜¯å¦è¿è¡ŒæŸ¥è¯¢è¯„ä¼°")
    return parser.parse_args()


def check_dirs(opt, root, mode, opt_path):
    """åˆ›å»ºç»“æœç›®å½•"""
    base_dir = os.path.join(opt.working_dir, opt.exp_name, root) if root else os.path.join(opt.working_dir, opt.exp_name)
    
    # æ ¹æ®æ¨¡å¼åˆ›å»ºä¸åŒçš„å­ç›®å½•
    mode_suffix = f"_{mode}" if mode != "build" else ""
    result_dir = os.path.join(base_dir, f"Results{mode_suffix}")
    config_dir = os.path.join(base_dir, f"Configs{mode_suffix}")
    metric_dir = os.path.join(base_dir, f"Metrics{mode_suffix}")
    
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(config_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)
    
    # å¤åˆ¶é…ç½®æ–‡ä»¶
    opt_name = opt_path[opt_path.rindex("/") + 1:]
    basic_name = os.path.join(opt_path.split("/")[0], "Config2.yaml")
    
    copyfile(opt_path, os.path.join(config_dir, opt_name))
    copyfile(basic_name, os.path.join(config_dir, "Config2.yaml"))
    
    return result_dir


def split_dataset_for_incremental(corpus, incremental_ratio=0.2):
    """
    å°†æ•°æ®é›†åˆ†ä¸ºåˆå§‹æ„å»ºé›†å’Œå¢é‡æ›´æ–°é›†
    
    Args:
        corpus: å®Œæ•´è¯­æ–™åº“
        incremental_ratio: å¢é‡æ•°æ®çš„æ¯”ä¾‹
        
    Returns:
        (initial_corpus, incremental_corpus): åˆå§‹è¯­æ–™åº“å’Œå¢é‡è¯­æ–™åº“
    """
    total_size = len(corpus)
    incremental_size = int(total_size * incremental_ratio)
    initial_size = total_size - incremental_size
    
    logger.info(f"æ•°æ®é›†åˆ’åˆ†: æ€»è®¡{total_size}, åˆå§‹{initial_size}, å¢é‡{incremental_size}")
    
    # ç®€å•æŒ‰é¡ºåºåˆ†å‰²ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç­–ç•¥
    initial_corpus = corpus[:initial_size]
    incremental_corpus = corpus[initial_size:]
    
    return initial_corpus, incremental_corpus


async def build_initial_graph(digimon, initial_corpus):
    """æ„å»ºåˆå§‹å›¾ç»“æ„"""
    logger.info(f"ğŸ—ï¸ å¼€å§‹æ„å»ºåˆå§‹å›¾ç»“æ„ï¼ŒåŒ…å«{len(initial_corpus)}ä¸ªæ–‡æ¡£")
    
    start_time = time.time()
    await digimon.insert(initial_corpus)
    build_time = time.time() - start_time
    
    # è·å–å›¾ç»Ÿè®¡ä¿¡æ¯
    if hasattr(digimon.graph, 'get_incremental_statistics'):
        stats = digimon.graph.get_incremental_statistics()
        logger.info(f"ğŸ“Š åˆå§‹å›¾æ„å»ºç»Ÿè®¡: {stats}")
    
    logger.info(f"âœ… åˆå§‹å›¾æ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.2f}ç§’")
    return build_time, stats if 'stats' in locals() else {}



async def insert_incremental_update(digimon, incremental_corpus):
    """æ‰§è¡Œè¯­æ–™æ’å…¥æ›´æ–°"""
    logger.info(f"ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°ï¼Œæ·»åŠ {len(incremental_corpus)}ä¸ªæ–°æ–‡æ¡£")
    
    if not hasattr(digimon.graph, 'insert_incremental'):
        logger.error("âŒ å½“å‰å›¾ç±»å‹ä¸æ”¯æŒå¢é‡æ›´æ–°")
        return None, {}
    
    start_time = time.time()
    
    try:
        # Step 1: ä½¿ç”¨ä¸“é—¨çš„å¢é‡æ›´æ–°æ–¹æ³•å¤„ç†chunkå­˜å‚¨
        logger.info("ğŸ“ å¢é‡æ›´æ–°chunkå­˜å‚¨ï¼ˆä¿æŠ¤ç°æœ‰æ•°æ®ï¼‰...")
        
        # ä½¿ç”¨æ–°çš„update_chunksæ–¹æ³•ï¼Œåªå¤„ç†æ–°æ–‡æ¡£ï¼Œä¸å½±å“ç°æœ‰chunk
        new_chunks = await digimon.doc_chunk.update_chunks(incremental_corpus)
        
        # Step 2: è·å–æ–°å¢çš„chunkæ•°æ®å¹¶æ‰§è¡Œå›¾å¢é‡æ›´æ–°
        if new_chunks:
            # è·å–æ‰€æœ‰chunkæ•°æ®ï¼Œæ‰¾å‡ºæ–°å¢çš„chunk
            all_chunks = await digimon.doc_chunk.get_chunks()
            new_chunk_items = []
            
            # æ ¹æ®new_chunksä¸­çš„chunk_idæ‰¾åˆ°å¯¹åº”çš„(key, TextChunk)å¯¹
            new_chunk_ids = {chunk["chunk_id"] for chunk in new_chunks}
            
            if all_chunks:
                for chunk_item in all_chunks:
                    if isinstance(chunk_item, tuple) and len(chunk_item) == 2:
                        chunk_key, chunk_obj = chunk_item
                        if chunk_key in new_chunk_ids:
                            new_chunk_items.append((chunk_key, chunk_obj))
            
            logger.info(f"ğŸ”§ æ‰§è¡Œå›¾å¢é‡æ›´æ–°ï¼Œå¤„ç†{len(new_chunk_items)}ä¸ªæ–°chunk...")
            success = await digimon.graph.insert_incremental(new_chunk_items)
        else:
            logger.info("ğŸ“ æ²¡æœ‰æ–°å¢chunkï¼Œè·³è¿‡å›¾æ›´æ–°")
            success = True
        #success = True
        update_time = time.time() - start_time
        
        if success:
            # è·å–æ›´æ–°åçš„ç»Ÿè®¡ä¿¡æ¯
            stats = digimon.graph.get_incremental_statistics()
            logger.info(f"ğŸ“Š è¯­æ–™æ’å…¥æ›´æ–°åç»Ÿè®¡: {stats}")
            logger.info(f"âœ… è¯­æ–™æ’å…¥æ›´æ–°æˆåŠŸï¼Œè€—æ—¶: {update_time:.2f}ç§’")
            return update_time, stats
        else:
            logger.error("âŒ è¯­æ–™æ’å…¥æ›´æ–°å¤±è´¥")
            return None, {}

    except Exception as e:
        logger.error(f"âŒ è¯­æ–™æ’å…¥æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None, {}

async def benchmark_incremental_vs_full(digimon, initial_corpus, incremental_corpus):
    """
    å¯¹æ¯”å¢é‡æ›´æ–°å’Œå…¨é‡é‡æ„çš„æ€§èƒ½
    """
    logger.info("ğŸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    results = {
        'initial_build': {},
        'incremental_update': {},
        'full_rebuild': {},
        'comparison': {}
    }
    
    # 1. æ„å»ºåˆå§‹å›¾
    logger.info("Step 1: æ„å»ºåˆå§‹å›¾")
    initial_time, initial_stats = await build_initial_graph(digimon, initial_corpus)
    results['initial_build'] = {
        'time': initial_time,
        'stats': initial_stats
    }
    
    # 2. ä¿å­˜åˆå§‹çŠ¶æ€ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
    if hasattr(digimon.graph._graph, 'save_metadata'):
        digimon.graph._graph.save_metadata({
            'stage': 'initial_build',
            'corpus_size': len(initial_corpus),
            'build_time': initial_time
        })
    
    # 3. æ‰§è¡Œå¢é‡æ›´æ–°
    logger.info("Step 2: æ‰§è¡Œå¢é‡æ›´æ–°")
    incremental_time, incremental_stats = await insert_incremental_update(digimon, incremental_corpus)
    if incremental_time is not None:
        results['incremental_update'] = {
            'time': incremental_time,
            'stats': incremental_stats
        }
    
    # 4. é‡æ–°æ„å»ºå›¾ï¼ˆå…¨é‡ï¼‰è¿›è¡Œå¯¹æ¯”
    logger.info("Step 3: å…¨é‡é‡æ„ç”¨äºå¯¹æ¯”")
    full_corpus = initial_corpus + incremental_corpus
    
    # æ¸…ç†ç°æœ‰å›¾
    if hasattr(digimon.graph, 'clear'):
        digimon.graph.clear()
    
    # å¼ºåˆ¶é‡æ„
    original_force = digimon.config.graph.force
    digimon.config.graph.force = True
    
    start_time = time.time()
    await digimon.insert(full_corpus)
    full_rebuild_time = time.time() - start_time
    
    # æ¢å¤åŸå§‹è®¾ç½®
    digimon.config.graph.force = original_force
    
    if hasattr(digimon.graph, 'get_incremental_statistics'):
        full_rebuild_stats = digimon.graph.get_incremental_statistics()
    else:
        full_rebuild_stats = {}
    
    results['full_rebuild'] = {
        'time': full_rebuild_time,
        'stats': full_rebuild_stats
    }
    
    # 5. è®¡ç®—å¯¹æ¯”ç»“æœ
    if incremental_time is not None:
        total_incremental_time = initial_time + incremental_time
        speedup = full_rebuild_time / total_incremental_time
        efficiency = (full_rebuild_time - total_incremental_time) / full_rebuild_time * 100
        
        results['comparison'] = {
            'total_incremental_time': total_incremental_time,
            'full_rebuild_time': full_rebuild_time,
            'speedup': speedup,
            'efficiency_improvement': efficiency,
            'time_saved': full_rebuild_time - total_incremental_time
        }
        
        logger.info(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        logger.info(f"   å¢é‡æ›´æ–°æ€»æ—¶é—´: {total_incremental_time:.2f}ç§’")
        logger.info(f"   å…¨é‡é‡æ„æ—¶é—´: {full_rebuild_time:.2f}ç§’")
        logger.info(f"   æ€§èƒ½æå‡: {speedup:.2f}x")
        logger.info(f"   æ•ˆç‡æå‡: {efficiency:.1f}%")
        logger.info(f"   èŠ‚çœæ—¶é—´: {full_rebuild_time - total_incremental_time:.2f}ç§’")
    
    return results


async def wrapper_query(query_dataset, digimon, result_dir, mode=""):
    """æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•"""
    all_res = []
    
    dataset_len = min(len(query_dataset), 3702)  # é™åˆ¶æµ‹è¯•æ•°é‡
    
    logger.info(f"ğŸ” å¼€å§‹æŸ¥è¯¢æµ‹è¯•ï¼Œæ¨¡å¼: {mode}, æµ‹è¯•{dataset_len}ä¸ªé—®é¢˜")
    
    for i in range(dataset_len):
        query = query_dataset[i]
        logger.info(f"æ­£åœ¨å¤„ç†é—®é¢˜ {i+1}/{dataset_len}...")
        
        try:
            res = await digimon.query(query["question"])
            query["output"] = res
            query["mode"] = mode  # æ ‡è®°æŸ¥è¯¢æ¨¡å¼
            all_res.append(query)
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ {i+1} å¤±è´¥: {e}")
            query["output"] = f"Error: {str(e)}"
            query["mode"] = mode
            all_res.append(query)
    
    # ä¿å­˜ç»“æœ
    all_res_df = pd.DataFrame(all_res)
    save_path = os.path.join(result_dir, f"results_{mode}.json" if mode else "results.json")
    all_res_df.to_json(save_path, orient="records", lines=True)
    
    logger.info(f"âœ… æŸ¥è¯¢æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {save_path}")
    return save_path


async def wrapper_evaluation(path, opt, result_dir, mode=""):
    """æ‰§è¡Œè¯„ä¼°"""
    try:
        eval = Evaluator(path, opt.dataset_name)
        res_dict = await eval.evaluate()
        
        save_path = os.path.join(result_dir, f"metrics_{mode}.json" if mode else "metrics.json")
        with open(save_path, "w") as f:
            json.dump(res_dict, f, indent=2)
        
        logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {save_path}")
        return res_dict
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        return {}


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è§£æé…ç½®
    opt = Config.parse(Path(args.opt), dataset_name=args.dataset_name)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡æ›´æ–°é…ç½®
    if opt.graph.graph_type != "hk_graph_tree_dynamic":
        logger.error(f"é”™è¯¯: é…ç½®æ–‡ä»¶çš„graph_typeåº”ä¸º'hk_graph_tree_dynamic'ï¼Œå½“å‰ä¸º'{opt.graph.graph_type}'")
        return
    
    # åˆ›å»ºç›®å½•
    result_dir = check_dirs(opt, args.root, args.mode, args.opt)
    
    # åˆ›å»ºGraphRAGå®ä¾‹
    digimon = GraphRAG(config=opt)
    
    # åŠ è½½æ•°æ®é›†
    query_dataset = RAGQueryDataset(
        data_dir=os.path.join(opt.data_root, opt.dataset_name)
    )
    corpus = query_dataset.get_corpus()
    logger.info(f"åŠ è½½æ•°æ®é›†: {len(corpus)} ä¸ªæ–‡æ¡£")
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒæ“ä½œ
    if args.mode == "build":
        # æ¨¡å¼1: ä»…æ„å»ºåˆå§‹å›¾
        logger.info("ğŸ—ï¸ æ¨¡å¼: æ„å»ºåˆå§‹å›¾")
        await build_initial_graph(digimon, corpus)
        
        if args.enable_query == "1":
            save_path = await wrapper_query(query_dataset, digimon, result_dir, "initial")
            await wrapper_evaluation(save_path, opt, result_dir, "initial")
    
    elif args.mode == "incremental":
        # æ¨¡å¼2: å¢é‡æ›´æ–°æµ‹è¯•
        logger.info("ğŸ”„ æ¨¡å¼: å¢é‡æ›´æ–°æµ‹è¯•")
        
        # åˆ†å‰²æ•°æ®é›†
        initial_corpus, incremental_corpus = split_dataset_for_incremental(
            corpus, args.incremental_ratio
        )
        
        # æ„å»ºåˆå§‹å›¾
        await build_initial_graph(digimon, initial_corpus)
        
        # æ‰§è¡Œå¢é‡æ›´æ–°
        await insert_incremental_update(digimon, incremental_corpus)
        
        if args.enable_query == "1":
            save_path = await wrapper_query(query_dataset, digimon, result_dir, "incremental")
            await wrapper_evaluation(save_path, opt, result_dir, "incremental")
    
    elif args.mode == "benchmark":
        # æ¨¡å¼3: æ€§èƒ½åŸºå‡†æµ‹è¯•
        logger.info("ğŸ æ¨¡å¼: æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # åˆ†å‰²æ•°æ®é›†
        initial_corpus, incremental_corpus = split_dataset_for_incremental(
            corpus, args.incremental_ratio
        )
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = await benchmark_incremental_vs_full(
            digimon, initial_corpus, incremental_corpus
        )
        
        # ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
        benchmark_path = os.path.join(result_dir, "benchmark_results.json")
        with open(benchmark_path, 'w') as f:
            json.dump(benchmark_results, f, indent=2)
        
        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœä¿å­˜åˆ°: {benchmark_path}")
        
        if args.enable_query == "1":
            save_path = await wrapper_query(query_dataset, digimon, result_dir, "benchmark")
            await wrapper_evaluation(save_path, opt, result_dir, "benchmark")
    
    elif args.mode == "query":
        # æ¨¡å¼4: ä»…æŸ¥è¯¢æµ‹è¯•ï¼ˆéœ€è¦å·²æœ‰å›¾ï¼‰
        logger.info("ğŸ” æ¨¡å¼: æŸ¥è¯¢æµ‹è¯•")
        
        # å°è¯•åŠ è½½ç°æœ‰å›¾
        if hasattr(digimon.graph, '_load_graph'):
            loaded = await digimon.graph._load_graph(force=False)
            if not loaded:
                logger.error("âŒ æœªæ‰¾åˆ°å·²æ„å»ºçš„å›¾ï¼Œè¯·å…ˆè¿è¡Œbuildæ¨¡å¼")
                return
        
        # å…³é”®ä¿®å¤ï¼šåŠ è½½ç°æœ‰çš„chunkæ•°æ®å’Œæ„å»ºæŸ¥è¯¢å™¨ä¸Šä¸‹æ–‡
        logger.info("ğŸ”§ åŠ è½½ç°æœ‰chunkæ•°æ®å’Œæ„å»ºæŸ¥è¯¢å™¨ä¸Šä¸‹æ–‡...")
        try:
            # åŠ è½½ç°æœ‰çš„chunkæ•°æ®ï¼ˆä¸é‡æ–°æ„å»ºï¼‰
            chunk_loaded = await digimon.doc_chunk._load_chunk(force=False)
            if not chunk_loaded:
                logger.error("âŒ æœªæ‰¾åˆ°å·²æœ‰çš„chunkæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´çš„å¢é‡æ›´æ–°")
                return
            logger.info("âœ… æˆåŠŸåŠ è½½ç°æœ‰chunkæ•°æ®")
            
            # å¦‚æœéœ€è¦å®ä½“é“¾æ¥æ˜ å°„ï¼ŒåŠ è½½ç°æœ‰çš„æ˜ å°„æ•°æ®
            if digimon.config.use_entity_link_chunk:
                await digimon.build_e2r_r2c_maps(force=False)
                logger.info("âœ… æˆåŠŸåŠ è½½å®ä½“é“¾æ¥æ˜ å°„æ•°æ®")
            
            # æ„å»ºæŸ¥è¯¢å™¨ä¸Šä¸‹æ–‡ï¼ˆå…³é”®æ­¥éª¤ï¼‰
            await digimon._build_retriever_context()
            logger.info("âœ… æŸ¥è¯¢å™¨ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæŸ¥è¯¢å™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return
        
        if args.enable_query == "1":
            save_path = await wrapper_query(query_dataset, digimon, result_dir, "query_only")
            await wrapper_evaluation(save_path, opt, result_dir, "query_only")
    
    logger.info("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())
